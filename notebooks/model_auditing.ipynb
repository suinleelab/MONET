{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e0567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import hydra\n",
    "import omegaconf\n",
    "import pyrootutils\n",
    "\n",
    "root = pyrootutils.setup_root(os.path.abspath(\"model_auditing.ipynb\"), pythonpath=True)\n",
    "import os\n",
    "\n",
    "os.chdir(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905ed405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(str(root / \"src\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e72b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib import gridspec\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import torch\n",
    "import tqdm\n",
    "from IPython import display\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b56d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp /System/Library/Fonts/Supplemental ~/.local/share/fonts/\n",
    "# rm -fr ~/.cache/matplotlib\n",
    "from matplotlib import font_manager\n",
    "from matplotlib.lines import Line2D\n",
    "from cycler import cycler\n",
    "from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
    "\n",
    "font_manager.findSystemFonts(fontpaths=None, fontext=\"ttf\")\n",
    "font_manager.findfont(\"Arial\") # Test with \"Special Elite\" too\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "\n",
    "plt.rcParams['legend.fancybox'] = False\n",
    "plt.rcParams['legend.edgecolor']='1.0'\n",
    "plt.rcParams['legend.framealpha']=0\n",
    "\n",
    "# https://github.com/dsc/colorbrewer-python/blob/master/colorbrewer.py\n",
    "\n",
    "Set1 = {\n",
    "    3: [[228,26,28], [55,126,184], [77,175,74]],\n",
    "    4: [[228,26,28], [55,126,184], [77,175,74], [152,78,163]],\n",
    "    5: [[228,26,28], [55,126,184], [77,175,74], [152,78,163], [255,127,0]],\n",
    "    6: [[228,26,28], [55,126,184], [77,175,74], [152,78,163], [255,127,0], [255,255,51]],\n",
    "    7: [[228,26,28], [55,126,184], [77,175,74], [152,78,163], [255,127,0], [255,255,51], [166,86,40]],\n",
    "    8: [[228,26,28], [55,126,184], [77,175,74], [152,78,163], [255,127,0], [255,255,51], [166,86,40], [247,129,191]],\n",
    "    9: [[228,26,28], [55,126,184], [77,175,74], [152,78,163], [255,127,0], [255,255,51], [166,86,40], [247,129,191], [153,153,153]],\n",
    "}\n",
    "\n",
    "Paired = {\n",
    "    3: [(166,206,227), [31,120,180], [178,223,138]],\n",
    "    4: [[166,206,227], [31,120,180], [178,223,138], [51,160,44]],\n",
    "    5: [[166,206,227], [31,120,180], [178,223,138], [51,160,44], [251,154,153]],\n",
    "    6: [[166,206,227], [31,120,180], [178,223,138], [51,160,44], [251,154,153], [227,26,28]],\n",
    "    7: [[166,206,227], [31,120,180], [178,223,138], [51,160,44], [251,154,153], [227,26,28], [253,191,111]],\n",
    "    8: [[166,206,227], [31,120,180], [178,223,138], [51,160,44], [251,154,153], [227,26,28], [253,191,111], [255,127,0]],\n",
    "    9: [[166,206,227], [31,120,180], [178,223,138], [51,160,44], [251,154,153], [227,26,28], [253,191,111], [255,127,0], [202,178,214]],\n",
    "    10: [[166,206,227], [31,120,180], [178,223,138], [51,160,44], [251,154,153], [227,26,28], [253,191,111], [255,127,0], [202,178,214], [106,61,154]],\n",
    "    11: [[166,206,227], [31,120,180], [178,223,138], [51,160,44], [251,154,153], [227,26,28], [253,191,111], [255,127,0], [202,178,214], [106,61,154], [255,255,153]],\n",
    "    12: [[166,206,227], [31,120,180], [178,223,138], [51,160,44], [251,154,153], [227,26,28], [253,191,111], [255,127,0], [202,178,214], [106,61,154], [255,255,153], [177,89,40]]\n",
    "}\n",
    "\n",
    "color_qual_7=['#F53345',\n",
    "            '#87D303',\n",
    "            '#04CBCC',\n",
    "            '#8650CD',\n",
    "            (160/256, 95/256, 0),\n",
    "            '#F5A637',              \n",
    "            '#DBD783',            \n",
    "             ]\n",
    "\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb738447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "import tqdm.contrib.concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e65fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MONET.datamodules.multiplex_datamodule import MultiplexDatamodule\n",
    "from MONET.utils.loader import custom_collate_per_key, dataloader_apply_func\n",
    "from MONET.utils.metrics import skincon_calcualte_auc_all\n",
    "from MONET.utils.static import (\n",
    "    concept_to_prompt,\n",
    "    fitzpatrick17k_disease_label,\n",
    "    fitzpatrick17k_ninelabel,\n",
    "    fitzpatrick17k_threelabel,\n",
    "    skincon_cols,\n",
    ")\n",
    "from MONET.utils.text_processing import generate_prompt_token_from_concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7824f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_to_exppath(wandb, log_path=\"/gscratch/cse/chanwkim/MONET_log/train/runs\"):\n",
    "    log_path = Path(log_path)\n",
    "    for experiment in os.listdir(log_path):\n",
    "        if os.path.exists(log_path / experiment / \"wandb\"):\n",
    "            filenames = os.listdir(log_path / experiment / \"wandb\")\n",
    "            filename = [filename for filename in filenames if filename.startswith(\"run\")][0][-8:]\n",
    "            if filename == wandb:\n",
    "                return log_path / experiment\n",
    "    raise RuntimeError(\"not found\")\n",
    "\n",
    "\n",
    "exppath = wandb_to_exppath(\n",
    "    wandb=\"baqqmm5v\", log_path=\"/projects/leelab2/chanwkim/dermatology_datasets/logs/train/runs\"\n",
    ")\n",
    "print([exppath / \"checkpoints\" / ckpt for ckpt in os.listdir(exppath / \"checkpoints/\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94eb6556",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_dict={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cc40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_dataloader(dataset_name):\n",
    "    if dataset_name==\"clinical_fd_clean_nodup\":\n",
    "        cfg_dm = omegaconf.OmegaConf.load(root / \"configs\" / \"datamodule\" / \"multiplex.yaml\")\n",
    "        # cfg.data_dir=\"/scr/chanwkim/dermatology_datasets\"\n",
    "        cfg_dm.data_dir = \"/sdata/chanwkim/dermatology_datasets\"\n",
    "        cfg_dm.dataset_name_test = \"clinical_fd_clean_nodup=all\"\n",
    "        cfg_dm.split_seed = 42\n",
    "\n",
    "        dm = hydra.utils.instantiate(cfg_dm)\n",
    "        dm.setup()     \n",
    "        \n",
    "        dataloader = dm.test_dataloader()      \n",
    "         \n",
    "        \n",
    "    elif dataset_name==\"isic\":\n",
    "        cfg_dm = omegaconf.OmegaConf.load(root / \"configs\" / \"datamodule\" / \"multiplex.yaml\")\n",
    "        # cfg.data_dir=\"/scr/chanwkim/dermatology_datasets\"\n",
    "        cfg_dm.data_dir = \"/sdata/chanwkim/dermatology_datasets\"\n",
    "        cfg_dm.dataset_name_test = \"isic=all\"\n",
    "        cfg_dm.split_seed = 42\n",
    "\n",
    "        dm = hydra.utils.instantiate(cfg_dm)\n",
    "        dm.setup()     \n",
    "        \n",
    "        dataloader = dm.test_dataloader()    \n",
    "        \n",
    "    return {\"dataloader\": dataloader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1025d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for dataset_name in [\"clinical_fd_clean_nodup\"\"isic\"]:  \n",
    "    variable_dict.setdefault(dataset_name, {})\n",
    "    variable_dict[dataset_name].update(setup_dataloader(dataset_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32959de",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1407964",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"zt0n2xd0\"\n",
    "model_device = \"cuda:6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac5a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_model = omegaconf.OmegaConf.load(root / \"configs\" / \"model\" / \"contrastive.yaml\")\n",
    "cfg_model.net.model_name_or_path = \"ViT-L/14\"\n",
    "cfg_model.net.device = model_device\n",
    "cfg_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913c9af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = hydra.utils.instantiate(cfg_model)\n",
    "model.to(model_device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03460efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path_dir = {\n",
    "    \"zt0n2xd0\": \"logs/train/runs/2023-01-17_20-58-15/checkpoints/last.ckpt\",\n",
    "}\n",
    "if model_name != \"ViT-L/14\":\n",
    "    model_path = model_path_dir[model_name]\n",
    "    loaded = torch.load(model_path, map_location=model_device)\n",
    "    model.load_state_dict(loaded[\"state_dict\"])\n",
    "    print('loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63c9eab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"zt0n2xd0\"\n",
    "model_device = \"cuda:5\"\n",
    "\n",
    "cfg_model = omegaconf.OmegaConf.load(root / \"configs\" / \"model\" / \"contrastive.yaml\")\n",
    "cfg_model.net.model_name_or_path = \"ViT-L/14\"\n",
    "cfg_model.net.device = model_device\n",
    "cfg_model\n",
    "\n",
    "model_vanilla = hydra.utils.instantiate(cfg_model)\n",
    "model_vanilla.to(model_device)\n",
    "model_vanilla.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a8b63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = Path(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531ee839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_func(batch):\n",
    "    with torch.no_grad():\n",
    "        batch[\"image\"] = batch[\"image\"].to(model_device)\n",
    "        image_features = model.model_step_with_image(batch)[\"image_features\"]\n",
    "        image_features_vanilla = model_vanilla.model_step_with_image(batch)[\"image_features\"]\n",
    "    # print(batch[\"metadata\"])\n",
    "    return {\n",
    "        \"image_features\": image_features.detach().cpu(),\n",
    "        \"image_features_vanilla\": image_features_vanilla.detach().cpu(),\n",
    "        \"metadata\": batch[\"metadata\"],\n",
    "    }\n",
    "\n",
    "def setup_features(dataset_name, dataloader):\n",
    "    if dataset_name==\"isic\":\n",
    "        image_features=torch.load(log_dir/\"image_features\"/\"isic.pt\")\n",
    "        metadata_all=dataloader.dataset.metadata_all\n",
    "\n",
    "        return {\"image_features\":image_features, \n",
    "#                 \"image_features_vanilla\":image_features_vanilla,\n",
    "                \"metadata_all\": metadata_all}\n",
    "    \n",
    "    else:\n",
    "        loader_applied = dataloader_apply_func(\n",
    "            dataloader=dataloader,\n",
    "            func=batch_func,\n",
    "            collate_fn=custom_collate_per_key,\n",
    "        )\n",
    "        image_features = loader_applied[\"image_features\"].cpu()\n",
    "        image_features_vanilla = loader_applied[\"image_features_vanilla\"].cpu()\n",
    "        metadata_all = loader_applied[\"metadata\"]\n",
    "\n",
    "        return {\"image_features\":image_features, \n",
    "                \"image_features_vanilla\":image_features_vanilla,\n",
    "                \"metadata_all\": metadata_all}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40e4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in [\"clinical_fd_clean_nodup\", \"isic\"]:\n",
    "    print(dataset_name)\n",
    "    variable_dict[dataset_name].update(setup_features(dataset_name, variable_dict[dataset_name][\"dataloader\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ac90ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gpustat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cba19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def get_layer_feature(model, feature_layer_name, image):\n",
    "    # image = self.normalize(self.toTensor(img)).unsqueeze(0).to(self.device)\n",
    "    # embedding = torch.zeros(image.shape[0], num_features, 1, 1).to(image.device)\n",
    "    feature_layer = model._modules.get(feature_layer_name)\n",
    "\n",
    "    embedding = []\n",
    "\n",
    "    def copyData(module, input, output):\n",
    "        embedding.append(output.data)\n",
    "\n",
    "    h = feature_layer.register_forward_hook(copyData)\n",
    "    out = model(image.to(image.device))\n",
    "    h.remove()\n",
    "    embedding = embedding[0]\n",
    "    assert embedding.shape[0] == image.shape[0], f\"{embedding.shape[0]} != {image.shape[0]}\"\n",
    "    assert embedding.shape[2] == 1, f\"{embedding.shape[2]} != 1\"\n",
    "    assert embedding.shape[2] == 1, f\"{embedding.shape[3]} != 1\"\n",
    "    return embedding[:, :, 0, 0]\n",
    "\n",
    "def batch_func_efficientnet(batch):\n",
    "    with torch.no_grad():\n",
    "        efficientnet_feature = get_layer_feature(\n",
    "            efficientnet, \"avgpool\", batch[\"image\"].to(efficientnet_device)\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        \"efficientnet_feature\": efficientnet_feature.detach().cpu(),\n",
    "        \"metadata\": batch[\"metadata\"],\n",
    "    }\n",
    "\n",
    "def setup_features_efficientnet(dataset_name, dataloader):\n",
    "    loader_applied = dataloader_apply_func(\n",
    "        dataloader=dataloader,\n",
    "        func=batch_func_efficientnet,\n",
    "        collate_fn=custom_collate_per_key,\n",
    "    )\n",
    "    image_features = loader_applied[\"efficientnet_feature\"].cpu()\n",
    "\n",
    "    return {\"efficientnet_feature\":image_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ed7e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_device=\"cuda:7\"\n",
    "efficientnet = torchvision.models.efficientnet_v2_s(\n",
    "    weights=torchvision.models.EfficientNet_V2_S_Weights.IMAGENET1K_V1\n",
    ").to(efficientnet_device)\n",
    "efficientnet.eval()\n",
    "\n",
    "for dataset_name in [\"clinical_fd_clean_nodup\", \"isic\"]:\n",
    "    print(dataset_name)\n",
    "    variable_dict[dataset_name].update(setup_features_efficientnet(dataset_name, variable_dict[dataset_name][\"dataloader\"])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_dict[\"clinical_fd_clean_nodup\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f13fec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnosis_malignant_mapping=\\\n",
    "{'AIMP':'indeterminate',\n",
    "'acrochordon':'benign',\n",
    "'actinic keratosis':'benign', # \n",
    "'angiofibroma or fibrous papule':'benign', \n",
    "'angiokeratoma':'benign',\n",
    "'angioma':'benign',\n",
    "'atypical melanocytic proliferation':'indeterminate',\n",
    "'atypical spitz tumor':'indeterminate', #\n",
    "'basal cell carcinoma':'malignant', #\n",
    "'cafe-au-lait macule':'benign',\n",
    "'clear cell acanthoma':'benign',\n",
    "'dermatofibroma':'benign', #\n",
    "'lentigo NOS':'benign',\n",
    "'lentigo simplex':'benign',\n",
    "'lichenoid keratosis':'benign',\n",
    "'melanoma':'malignant',\n",
    "'melanoma metastasis':'malignant',\n",
    "'neurofibroma':'benign',\n",
    "'nevus':'benign',\n",
    "'other':'indeterminate',\n",
    "'pigmented benign keratosis':'benign', #??\n",
    "'scar':'benign',\n",
    "'seborrheic keratosis':'benign',\n",
    "'solar lentigo':'benign',\n",
    "'squamous cell carcinoma':'malignant',\n",
    "'vascular lesion':'unknown', # \n",
    "'verruca':'benign'\n",
    "}\n",
    "\n",
    "def map_diagnosis_malignant(diagnosis, benign_malignant):\n",
    "#     if diagnosis==\"basal cell carcinoma\":\n",
    "#         print(diagnosis_malignant_mapping[diagnosis], benign_malignant)    \n",
    "    if isinstance(benign_malignant, str):\n",
    "        return benign_malignant\n",
    "    elif diagnosis in diagnosis_malignant_mapping.keys():\n",
    "        return diagnosis_malignant_mapping[diagnosis]\n",
    "    elif np.isnan(diagnosis):\n",
    "        return \"indeterminate\"\n",
    "    else:\n",
    "        raise RuntimeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacdd666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(dataset_name, metadata_all):\n",
    "    if \"clinical_fd_clean\" in dataset_name:\n",
    "        y_pos=(((metadata_all[\"source\"]==\"fitz\")&(metadata_all[\"three_partition_label\"]==\"malignant\"))|\n",
    "              ((metadata_all[\"source\"]==\"ddi\")&(metadata_all[\"malignant\"] == True))).values\n",
    "        \n",
    "        valid_idx=(metadata_all[\"skincon_Do not consider this image\"]!=1).values\n",
    "        \n",
    "        concept_list=skincon_cols\n",
    "        \n",
    "        \n",
    "    elif dataset_name==\"isic\":  \n",
    "        metadata_all[\"benign_malignant_full\"]=\\\n",
    "        metadata_all.apply(lambda x: map_diagnosis_malignant(x[\"diagnosis\"], x[\"benign_malignant\"]), axis=1)\n",
    "        #metadata_all[\"benign_malignant_full\"].value_counts()\n",
    "        #metadata_all.groupby(\"diagnosis\").apply(lambda x: x[\"benign_malignant_full\"].value_counts())\n",
    "        metadata_all[\"benign_malignant_bool\"]=metadata_all[\"benign_malignant_full\"].str.contains(\"malignant\")\n",
    "        \n",
    "        y_pos=metadata_all[\"benign_malignant_bool\"].values\n",
    "        \n",
    "        valid_idx = (metadata_all[\"benign_malignant_full\"].str.contains(\"malignant\")|metadata_all[\"benign_malignant_full\"].str.contains(\"benign\")).values\n",
    "        \n",
    "        concept_list=skincon_cols+\\\n",
    "                            [\"purple pen\", \n",
    "                             \"nail\", \n",
    "                             \"pinkish\", \n",
    "                             \"red\", \n",
    "                             \"hair\", \n",
    "                             \"orange sticker\", \n",
    "                             \"dermoscope border\",\n",
    "                             \"gel\",\n",
    "                             \"malignant\",\n",
    "                             \"melanoma\"]      \n",
    "        \n",
    "        concept_list+=concept_list+\\\n",
    "                        [f\"disease_{disease_name}\" for disease_name in ['seborrheic keratosis', 'nevus', 'squamous cell carcinoma',\n",
    "                        'melanoma', 'lichenoid keratosis', 'lentigo',\n",
    "                        'actinic keratosis', 'basal cell carcinoma', 'dermatofibroma',\n",
    "                        'atypical melanocytic proliferation', 'verruca',\n",
    "                        'clear cell acanthoma', 'angiofibroma or fibrous papule', 'scar',\n",
    "                        'angioma', 'atypical spitz tumor', 'solar lentigo', 'AIMP',\n",
    "                        'neurofibroma', 'lentigo simplex', 'acrochordon', \n",
    "                        'angiokeratoma', 'vascular lesion', 'cafe-au-lait macule',\n",
    "                        'pigmented benign keratosis']]\n",
    "        \n",
    "    return {\"valid_idx\": valid_idx,\n",
    "            \"y_pos\": y_pos,\n",
    "            \"metadata_all\": metadata_all,\n",
    "            \"concept_list\": concept_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480d422",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in [\"clinical_fd_clean_nodup\", \"isic\"]:\n",
    "    variable_dict[dataset_name].update(\n",
    "        set_config(dataset_name, variable_dict[dataset_name][\"metadata_all\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embedding(dataset_name, image_features):\n",
    "    #prompt_ref_tokenized = clip.tokenize(prompt_ref, truncate=True)\n",
    "    #output = model.model_step_with_text({\"text\": prompt_ref_tokenized.to(model_device)})\n",
    "    #prompt_ref_embedding=output[\"text_features\"].detach().cpu()\n",
    "    #prompt_ref_embedding_norm=prompt_ref_embedding/prompt_ref_embedding.norm(dim=1, keepdim=True)      \n",
    "    \n",
    "    image_features_norm = image_features / image_features.norm(dim=1, keepdim=True)\n",
    "    \n",
    "    return {\"image_features_norm\": image_features_norm}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632c238",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in [\"clinical_fd_clean_nodup\", \"isic\"]:\n",
    "    if dataset_name==\"clinical_fd_clean_nodup\":\n",
    "        variable_dict[dataset_name].update(\n",
    "            {\"image_features_vanilla_norm\":normalize_embedding(dataset_name, \n",
    "                            variable_dict[dataset_name][\"image_features_vanilla\"])[\"image_features_norm\"]}\n",
    "        )           \n",
    "    variable_dict[dataset_name].update(\n",
    "        {\"image_features_norm\":normalize_embedding(dataset_name, \n",
    "                        variable_dict[dataset_name][\"image_features\"])[\"image_features_norm\"]}\n",
    "    )  \n",
    "#     variable_dict[dataset_name].update(\n",
    "#         {\"image_features_vanilla_norm\":normalize_embedding(dataset_name, \n",
    "#                         variable_dict[dataset_name][\"image_features_vanilla\"])[\"image_features_norm\"]}\n",
    "#     )    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484878d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_embedding(dataset_name, concept_list, model):\n",
    "    prompt_info={}\n",
    "    \n",
    "    for concept_name in concept_list:\n",
    "        if dataset_name==\"clinical_fd_clean_nodup\":\n",
    "            prompt_dict, text_counter = concept_to_prompt(concept_name[8:])\n",
    "            prompt_engineered_list = []\n",
    "            for k, v in prompt_dict.items():\n",
    "                if k != \"original\":\n",
    "                    prompt_engineered_list += v    \n",
    "            concept_term_list = list(set([prompt.replace(\"This is \", \"\").replace(\"This photo is \", \"\").replace(\"This lesion is \", \"\").replace(\"skin has become \", \"\").lower()\n",
    "                                      for prompt in prompt_engineered_list]))\n",
    "            prompt_template_list=[\"This is skin image of {}\", \"This is dermatology image of {}\", \"This is image of {}\"]\n",
    "            #prompt_target=[prompt_template.format(term) for prompt_template in prompt_template_list for term in concept_term_list]            \n",
    "            prompt_target=[[prompt_template.format(term) for term in concept_term_list] for prompt_template in prompt_template_list]\n",
    "            \n",
    "            prompt_ref = [[\"This is skin image\"], [\"This is dermatology image\"], [\"This is image\"]]        \n",
    "        \n",
    "        elif dataset_name==\"isic\":\n",
    "            if concept_name.startswith(\"skincon_\"):\n",
    "                prompt_dict, text_counter = concept_to_prompt(concept_name[8:])\n",
    "                prompt_engineered_list = []\n",
    "                for k, v in prompt_dict.items():\n",
    "                    if k != \"original\":\n",
    "                        prompt_engineered_list += v\n",
    "\n",
    "                concept_term_list = list(set([prompt.replace(\"This is \", \"\").replace(\"This photo is \", \"\").replace(\"This lesion is \", \"\").replace(\"skin has become \", \"\").lower()\n",
    "                                          for prompt in prompt_engineered_list]))\n",
    "                prompt_template_list=[\"This is dermatoscopy of {}\", \"This is dermoscopy of {}\"]\n",
    "                prompt_target=[prompt_template.format(term) for prompt_template in prompt_template_list for term in concept_term_list]\n",
    "                prompt_ref = [\"This is dermatoscopy\", \"This is dermoscopy\"]\n",
    "                prompt_target=[[prompt_template.format(term) for term in concept_term_list] for prompt_template in prompt_template_list]\n",
    "                prompt_ref = [[\"This is dermatoscopy\"], [\"This is dermoscopy\"]] \n",
    "                \n",
    "                prompt_target=[[prompt_template.format(term) for prompt_template in prompt_template_list for term in concept_term_list]]\n",
    "                prompt_ref = [[\"This is dermatoscopy\", \"This is dermoscopy\"]] \n",
    "            elif concept_name.startswith(\"disease_\"):  \n",
    "                if concept_name==\"disease_AIMP\":\n",
    "                    disease_name=concept_name[8:]\n",
    "                    prompt_target=[[\"This is dermatoscopy of AIMP\",\n",
    "                                    \"This is dermatoscopy of Atypical intraepidermal melanocytic proliferation\"],\n",
    "                                   [\"This is dermoscopy of AIMP\",\n",
    "                                    \"This is dermoscopy of Atypical intraepidermal melanocytic proliferation\"]]\n",
    "                    prompt_ref = [[\"This is dermatoscopy\"], [\"This is dermoscopy\"]]\n",
    "                else:\n",
    "                    disease_name=concept_name[8:]\n",
    "                    prompt_target=[[f\"This is dermatoscopy of {disease_name}\"],\n",
    "                                   [f\"This is dermoscopy of {disease_name}\"]] \n",
    "                    prompt_ref = [[\"This is dermatoscopy\"], [\"This is dermoscopy\"]]\n",
    "                \n",
    "            else:\n",
    "                if concept_name==\"gel\":\n",
    "                    #concept_term_list=[\"water drop\", 'gel', \"fluid\"]\n",
    "                    prompt_target=[[\"This is dermatoscopy of water drop\", \"This is dermatoscopy of gel\", \"This is dermatoscopy of dermoscopy liquid\"],\n",
    "                                   [\"This is dermoscopy of water drop\", \"This is dermoscopy of gel\", \"This is dermoscopy of dermoscopy liquid\"],\n",
    "                                  ]\n",
    "                    prompt_target=[[\"This is dermatoscopy of gel\"],\n",
    "                                   [\"This is dermoscopy of gel\"],\n",
    "                                  ]                    \n",
    "                    \n",
    "                    prompt_ref = [[\"This is dermatoscopy\"], \n",
    "                                  [\"This is dermoscopy\"]]\n",
    "                elif concept_name==\"dermoscope border\":\n",
    "                    concept_term_list=[\"dermoscope\"]\n",
    "                    prompt_target=[\"This is hole\"]\n",
    "                    prompt_target=[\"This is scope hole\", \"This is circle\", \"This is dermoscope\"]\n",
    "                    #prompt_target=[[\"This is dermatoscopy of dermoscope\", \"This is dermatoscopy of dermoscopy\"]]\n",
    "                    prompt_target=[[\"This is dermatoscopy of dermoscopy\"]]\n",
    "                    prompt_ref = [[\"This is dermatoscopy\"]]\n",
    "                    \n",
    "                else:\n",
    "                    concept_term_list=[concept_name]\n",
    "                    prompt_template_list=[\"This is dermatoscopy of {}\", \"This is dermoscopy of {}\"]\n",
    "                    prompt_target=[prompt_template.format(term) for prompt_template in prompt_template_list for term in concept_term_list]\n",
    "                    \n",
    "                    prompt_ref = [[\"This is dermatoscopy\"], [\"This is dermoscopy\"]]\n",
    "                \n",
    "        \n",
    "        #print(prompt_target, prompt_ref)\n",
    "        # target embedding\n",
    "        prompt_target_tokenized=[clip.tokenize(prompt_list, truncate=True) for prompt_list in prompt_target]\n",
    "        with torch.no_grad():\n",
    "            prompt_target_embedding = torch.stack([model.model_step_with_text({\"text\": prompt_tokenized.to(model_device)})[\n",
    "                    \"text_features\"].detach().cpu() for prompt_tokenized in prompt_target_tokenized])\n",
    "        prompt_target_embedding_norm=prompt_target_embedding/prompt_target_embedding.norm(dim=2, keepdim=True)          \n",
    "\n",
    "        # reference embedding\n",
    "        prompt_ref_tokenized=[clip.tokenize(prompt_list, truncate=True) for prompt_list in prompt_ref]\n",
    "        with torch.no_grad():\n",
    "            prompt_ref_embedding = torch.stack([model.model_step_with_text({\"text\": prompt_tokenized.to(model_device)})[\n",
    "                    \"text_features\"].detach().cpu() for prompt_tokenized in prompt_ref_tokenized])\n",
    "        prompt_ref_embedding_norm=prompt_ref_embedding/prompt_ref_embedding.norm(dim=2, keepdim=True)                \n",
    "\n",
    "        prompt_info[concept_name]={\"prompt_ref_embedding_norm\":prompt_ref_embedding_norm,\n",
    "                                   \"prompt_target_embedding_norm\":prompt_target_embedding_norm,\n",
    "                                  }\n",
    "        print(dataset_name, concept_name, prompt_target, prompt_ref)\n",
    "        print(prompt_target_embedding_norm)\n",
    "        print(prompt_ref_embedding_norm)\n",
    "        del prompt_ref\n",
    "        del prompt_target\n",
    "    \n",
    "    return {\"prompt_info\": prompt_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc7ef53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in [\"clinical_fd_clean_nodup\", \"isic\"]:\n",
    "    if dataset_name==\"clinical_fd_clean_nodup\":\n",
    "        variable_dict[dataset_name].update(\n",
    "            {\"prompt_info_vanilla\":get_concept_embedding(dataset_name, \n",
    "                          concept_list=variable_dict[dataset_name][\"concept_list\"],\n",
    "                                 model=model_vanilla)[\"prompt_info\"]})            \n",
    "    variable_dict[dataset_name].update(\n",
    "        {\"prompt_info\":get_concept_embedding(dataset_name, \n",
    "                      concept_list=variable_dict[dataset_name][\"concept_list\"],\n",
    "                             model=model)[\"prompt_info\"]})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0367e25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similaity_score(image_features_norm, \n",
    "                              prompt_target_embedding_norm,\n",
    "                              prompt_ref_embedding_norm,\n",
    "                              temp=1,\n",
    "                              normalize=True):\n",
    "\n",
    "    target_similarity=prompt_target_embedding_norm.float()@image_features_norm.T.float()\n",
    "    ref_similarity=prompt_ref_embedding_norm.float()@image_features_norm.T.float()\n",
    "    \n",
    "    \n",
    "    target_similarity_mean=target_similarity.mean(dim=[1])\n",
    "    ref_similarity_mean=ref_similarity.mean(axis=1)\n",
    "         \n",
    "    if normalize:\n",
    "        similarity_score=scipy.special.softmax([target_similarity_mean.numpy()/temp, \n",
    "                            ref_similarity_mean.numpy()/temp], axis=0)[0,:].mean(axis=0)   \n",
    "    else:\n",
    "        similarity_score=target_similarity_mean.mean(axis=0)\n",
    "\n",
    "    \n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa48057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from MONET.datamodules.components.base_dataset import BaseDataset\n",
    "\n",
    "def get_training_data(dataloader, metadata_all, valid_idx, y_pos, subset_idx_train, subset_idx_test, n_px=None):\n",
    "    metadata_all_new = dataloader.dataset.metadata_all.copy()\n",
    "    metadata_all_new[\"label\"]=y_pos.astype(int)\n",
    "    metadata_all_new_train=metadata_all_new[valid_idx&subset_idx_train]\n",
    "    # metadata_all_new=metadata_all_new.iloc[list(true_set.union(false_set))]\n",
    "\n",
    "    train_idx, val_idx = train_test_split(metadata_all_new_train.index, test_size=0.2, random_state=42)\n",
    "    \n",
    "    print(\"train:\", len(metadata_all_new_train))\n",
    "\n",
    "    if n_px is None:\n",
    "        n_px=dataloader.dataset.n_px\n",
    "    \n",
    "    data_train = BaseDataset(\n",
    "        image_path_or_binary_dict=dataloader.dataset.image_path_dict,\n",
    "        n_px=n_px,\n",
    "        norm_mean=dataloader.dataset.transforms_aftertensor.transforms[1].mean,\n",
    "        norm_std=dataloader.dataset.transforms_aftertensor.transforms[1].std,\n",
    "        augment=False,\n",
    "        metadata_all=metadata_all_new_train.loc[train_idx],\n",
    "        integrity_level=\"weak\",\n",
    "        return_label=[\"label\"],\n",
    "    )\n",
    "\n",
    "    data_val = BaseDataset(\n",
    "        image_path_or_binary_dict=dataloader.dataset.image_path_dict,\n",
    "        n_px=n_px,\n",
    "        norm_mean=dataloader.dataset.transforms_aftertensor.transforms[1].mean,\n",
    "        norm_std=dataloader.dataset.transforms_aftertensor.transforms[1].std,\n",
    "        augment=False,\n",
    "        metadata_all=metadata_all_new_train.loc[val_idx],\n",
    "        integrity_level=\"weak\",\n",
    "        return_label=[\"label\"],\n",
    "    )\n",
    "    \n",
    "    data_test = BaseDataset(\n",
    "        image_path_or_binary_dict=dataloader.dataset.image_path_dict,\n",
    "        n_px=n_px,\n",
    "        norm_mean=dataloader.dataset.transforms_aftertensor.transforms[1].mean,\n",
    "        norm_std=dataloader.dataset.transforms_aftertensor.transforms[1].std,\n",
    "        augment=False,\n",
    "        metadata_all=metadata_all_new[valid_idx&subset_idx_test],\n",
    "        integrity_level=\"weak\",\n",
    "        return_label=[\"label\"],\n",
    "    )    \n",
    "\n",
    "    from MONET.utils.loader import custom_collate\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=data_train,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "#         pin_memory=True,\n",
    "#         persistent_workers=False,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate,\n",
    "    )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=data_val,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "#         pin_memory=True,\n",
    "#         persistent_workers=False,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate,\n",
    "    )   \n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=data_val,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "#         pin_memory=True,\n",
    "#         persistent_workers=False,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate,\n",
    "    )       \n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8934c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchmetrics import AUROC\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet50(weights=\"ResNet50_Weights.IMAGENET1K_V1\")\n",
    "\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "            # pass\n",
    "\n",
    "        head_in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.head = nn.Linear(head_in_features, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.head(x)\n",
    "        return x\n",
    "    \n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, output_dim):\n",
    "        super().__init__()\n",
    "        self.inception = torchvision.models.inception_v3(weights=\"Inception_V3_Weights.IMAGENET1K_V1\")\n",
    "        self.fc = nn.Linear(2048, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_to_representation(x)\n",
    "        x = self.fc(x)\n",
    "        # N x 200\n",
    "        return x\n",
    "\n",
    "    def input_to_representation(self, x):\n",
    "        # N x 3 x 299 x 299\n",
    "        x = self.inception.Conv2d_1a_3x3(x)\n",
    "        # N x 32 x 149 x 149\n",
    "        x = self.inception.Conv2d_2a_3x3(x)\n",
    "        # N x 32 x 147 x 147\n",
    "        x = self.inception.Conv2d_2b_3x3(x)\n",
    "        # N x 64 x 147 x 147\n",
    "        x = self.inception.maxpool1(x)\n",
    "        # N x 64 x 73 x 73\n",
    "        x = self.inception.Conv2d_3b_1x1(x)\n",
    "        # N x 80 x 73 x 73\n",
    "        x = self.inception.Conv2d_4a_3x3(x)\n",
    "        # N x 192 x 71 x 71\n",
    "        x = self.inception.maxpool2(x)\n",
    "        # N x 192 x 35 x 35\n",
    "        x = self.inception.Mixed_5b(x)\n",
    "        # N x 256 x 35 x 35\n",
    "        x = self.inception.Mixed_5c(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.inception.Mixed_5d(x)\n",
    "        # N x 288 x 35 x 35\n",
    "        x = self.inception.Mixed_6a(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.inception.Mixed_6b(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.inception.Mixed_6c(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.inception.Mixed_6d(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.inception.Mixed_6e(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        aux: Optional[torch.Tensor] = None\n",
    "        if self.inception.AuxLogits is not None:\n",
    "            if self.inception.training:\n",
    "                aux = self.inception.AuxLogits(x)\n",
    "        # N x 768 x 17 x 17\n",
    "        x = self.inception.Mixed_7a(x)\n",
    "        # N x 1280 x 8 x 8\n",
    "        x = self.inception.Mixed_7b(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        x = self.inception.Mixed_7c(x)\n",
    "        # N x 2048 x 8 x 8\n",
    "        # Adaptive average pooling\n",
    "        x = self.inception.avgpool(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = self.inception.dropout(x)\n",
    "        # N x 2048 x 1 x 1\n",
    "        x = torch.flatten(x, 1)\n",
    "        # N x 2048\n",
    "        return x    \n",
    "\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "def find_thres_best_f1(y_test, y_test_pred):\n",
    "    precision, recall, thresholds = sklearn.metrics.precision_recall_curve(y_test, y_test_pred)\n",
    "    numerator = 2 * recall * precision\n",
    "    denom = recall + precision\n",
    "    f1_scores = np.divide(numerator, denom, out=np.zeros_like(denom), where=(denom!=0))\n",
    "    max_f1 = np.max(f1_scores)\n",
    "    max_f1_thresh = thresholds[np.argmax(f1_scores)]    \n",
    "    return max_f1_thresh \n",
    "\n",
    "def train_classifier(train_dataloader, val_dataloader, test_dataloader, classifier_type=\"resnet\", verbose=True):\n",
    "    if classifier_type==\"resnet\":\n",
    "        classifier = Classifier(output_dim=1)\n",
    "    elif classifier_type==\"inception\":\n",
    "        classifier = Inception(output_dim=1)\n",
    "    classifier_device = \"cuda:5\"\n",
    "    classifier.to(classifier_device)\n",
    "    optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=2, verbose=True)\n",
    "    early_stopper = EarlyStopper(patience=5, min_delta=0)\n",
    "\n",
    "    train_auroc = AUROC(task=\"binary\")\n",
    "    val_auroc = AUROC(task=\"binary\")\n",
    "    for epoch in range(20):\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        classifier.train()\n",
    "        if verbose:\n",
    "            pbar=tqdm.tqdm(train_dataloader)\n",
    "        else:\n",
    "            pbar=train_dataloader        \n",
    "        for batch in pbar:\n",
    "            image, label = batch[\"image\"].to(classifier_device), batch[\"label\"].to(classifier_device)\n",
    "            logits = classifier(image)\n",
    "            weight = torch.ones(label.shape[0], device=label.device)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                input=logits[:, 0], target=(label == 1).float()\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * image.size(0)\n",
    "            train_auroc.update(logits, (label == 1))\n",
    "\n",
    "        val_loss = 0\n",
    "        classifier.eval()\n",
    "        label_list=[]\n",
    "        logits_list=[]\n",
    "        with torch.no_grad():   \n",
    "            if verbose:\n",
    "                pbar=tqdm.tqdm(val_dataloader)\n",
    "            else:\n",
    "                pbar=val_dataloader             \n",
    "            for batch in pbar:\n",
    "                image, label = batch[\"image\"].to(classifier_device), batch[\"label\"].to(\n",
    "                    classifier_device\n",
    "                )\n",
    "                logits = classifier(image)\n",
    "                loss = F.binary_cross_entropy_with_logits(\n",
    "                    input=logits[:, 0], target=(label == 1).float()\n",
    "                )\n",
    "                val_loss += loss.item() * image.size(0)\n",
    "                val_auroc.update(logits, (label == 1))\n",
    "                logits_list.append(logits.detach().cpu().numpy())\n",
    "                label_list.append(label.detach().cpu().numpy())                \n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Train loss: {train_loss/len(train_dataloader.dataset):.3f} AUROC: {train_auroc.compute():.3f} Val loss: {val_loss/len(val_dataloader.dataset):.3f} AUROC: {val_auroc.compute():.3f}\"\n",
    "            )\n",
    "        scheduler.step(val_loss)\n",
    "        if early_stopper.early_stop(val_loss):\n",
    "            print(\"break\")\n",
    "            break\n",
    "        train_auroc.reset()\n",
    "        val_auroc.reset() \n",
    "        max_f1_thresh=find_thres_best_f1(y_test=np.hstack(label_list), y_test_pred=np.concatenate(logits_list)[:,0])\n",
    "        print(max_f1_thresh)\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "    test_auroc = AUROC(task=\"binary\")    \n",
    "    test_loss = 0\n",
    "    classifier.eval()\n",
    "    \n",
    "    logits_list=[]\n",
    "    label_list=[]\n",
    "    metadata_list=[]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        if verbose:\n",
    "            pbar=tqdm.tqdm(test_dataloader)\n",
    "        else:\n",
    "            pbar=test_dataloader          \n",
    "        for batch in tqdm.tqdm(test_dataloader):\n",
    "            image, label = batch[\"image\"].to(classifier_device), batch[\"label\"].to(\n",
    "                classifier_device\n",
    "            )\n",
    "            logits = classifier(image)\n",
    "            loss = F.binary_cross_entropy_with_logits(\n",
    "                input=logits[:, 0], target=(label == 1).float()\n",
    "            )\n",
    "            test_loss += loss.item() * image.size(0)\n",
    "            test_auroc.update(logits, (label == 1))\n",
    "            logits_list.append(logits.detach().cpu().numpy())\n",
    "            label_list.append(label.detach().cpu().numpy())\n",
    "            metadata_list.append(batch[\"metadata\"])\n",
    "            \n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Test loss: {test_loss/len(test_dataloader.dataset):.3f} AUROC: {test_auroc.compute():.3f}\"\n",
    "        )   \n",
    "    return test_auroc.compute(), classifier, logits_list, label_list, metadata_list, max_f1_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5574f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdrcorrection(pvals, alpha=0.05, method='indep', is_sorted=False):\n",
    "    '''\n",
    "    pvalue correction for false discovery rate.\n",
    "\n",
    "    This covers Benjamini/Hochberg for independent or positively correlated and\n",
    "    Benjamini/Yekutieli for general or negatively correlated tests.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pvals : array_like, 1d\n",
    "        Set of p-values of the individual tests.\n",
    "    alpha : float, optional\n",
    "        Family-wise error rate. Defaults to ``0.05``.\n",
    "    method : {'i', 'indep', 'p', 'poscorr', 'n', 'negcorr'}, optional\n",
    "        Which method to use for FDR correction.\n",
    "        ``{'i', 'indep', 'p', 'poscorr'}`` all refer to ``fdr_bh``\n",
    "        (Benjamini/Hochberg for independent or positively\n",
    "        correlated tests). ``{'n', 'negcorr'}`` both refer to ``fdr_by``\n",
    "        (Benjamini/Yekutieli for general or negatively correlated tests).\n",
    "        Defaults to ``'indep'``.\n",
    "    is_sorted : bool, optional\n",
    "        If False (default), the p_values will be sorted, but the corrected\n",
    "        pvalues are in the original order. If True, then it assumed that the\n",
    "        pvalues are already sorted in ascending order.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    rejected : ndarray, bool\n",
    "        True if a hypothesis is rejected, False if not\n",
    "    pvalue-corrected : ndarray\n",
    "        pvalues adjusted for multiple hypothesis testing to limit FDR\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    If there is prior information on the fraction of true hypothesis, then alpha\n",
    "    should be set to ``alpha * m/m_0`` where m is the number of tests,\n",
    "    given by the p-values, and m_0 is an estimate of the true hypothesis.\n",
    "    (see Benjamini, Krieger and Yekuteli)\n",
    "\n",
    "    The two-step method of Benjamini, Krieger and Yekutiel that estimates the number\n",
    "    of false hypotheses will be available (soon).\n",
    "\n",
    "    Both methods exposed via this function (Benjamini/Hochberg, Benjamini/Yekutieli)\n",
    "    are also available in the function ``multipletests``, as ``method=\"fdr_bh\"`` and\n",
    "    ``method=\"fdr_by\"``, respectively.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    multipletests\n",
    "\n",
    "    '''\n",
    "    \n",
    "\n",
    "    def _ecdf(x):\n",
    "        '''no frills empirical cdf used in fdrcorrection\n",
    "        '''\n",
    "        nobs = len(x)\n",
    "        return np.arange(1,nobs+1)/float(nobs)    \n",
    "\n",
    "    pvals = np.asarray(pvals)\n",
    "    assert pvals.ndim == 1, \"pvals must be 1-dimensional, that is of shape (n,)\"\n",
    "\n",
    "    if not is_sorted:\n",
    "        pvals_sortind = np.argsort(pvals)\n",
    "        pvals_sorted = np.take(pvals, pvals_sortind)\n",
    "    else:\n",
    "        pvals_sorted = pvals  # alias\n",
    "\n",
    "    if method in ['i', 'indep', 'p', 'poscorr']:\n",
    "        ecdffactor = _ecdf(pvals_sorted)\n",
    "    elif method in ['n', 'negcorr']:\n",
    "        cm = np.sum(1./np.arange(1, len(pvals_sorted)+1))   #corrected this\n",
    "        ecdffactor = _ecdf(pvals_sorted) / cm\n",
    "##    elif method in ['n', 'negcorr']:\n",
    "##        cm = np.sum(np.arange(len(pvals)))\n",
    "##        ecdffactor = ecdf(pvals_sorted)/cm\n",
    "    else:\n",
    "        raise ValueError('only indep and negcorr implemented')\n",
    "    reject = pvals_sorted <= ecdffactor*alpha\n",
    "    if reject.any():\n",
    "        rejectmax = max(np.nonzero(reject)[0])\n",
    "        reject[:rejectmax] = True\n",
    "\n",
    "    pvals_corrected_raw = pvals_sorted / ecdffactor\n",
    "    pvals_corrected = np.minimum.accumulate(pvals_corrected_raw[::-1])[::-1]\n",
    "    del pvals_corrected_raw\n",
    "    pvals_corrected[pvals_corrected>1] = 1\n",
    "    if not is_sorted:\n",
    "        pvals_corrected_ = np.empty_like(pvals_corrected)\n",
    "        pvals_corrected_[pvals_sortind] = pvals_corrected\n",
    "        del pvals_corrected\n",
    "        reject_ = np.empty_like(reject)\n",
    "        reject_[pvals_sortind] = reject\n",
    "        return reject_, pvals_corrected_\n",
    "    else:\n",
    "        return reject, pvals_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb33ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import xlogy\n",
    "def log_loss(\n",
    "    y_true, y_pred, *, eps=\"auto\", normalize=True, sample_weight=None, labels=None\n",
    "):\n",
    "    \n",
    "    r\"\"\"Log loss, aka logistic loss or cross-entropy loss.\n",
    "\n",
    "    This is the loss function used in (multinomial) logistic regression\n",
    "    and extensions of it such as neural networks, defined as the negative\n",
    "    log-likelihood of a logistic model that returns ``y_pred`` probabilities\n",
    "    for its training data ``y_true``.\n",
    "    The log loss is only defined for two or more labels.\n",
    "    For a single sample with true label :math:`y \\in \\{0,1\\}` and\n",
    "    a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log\n",
    "    loss is:\n",
    "\n",
    "    .. math::\n",
    "        L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n",
    "\n",
    "    Read more in the :ref:`User Guide <log_loss>`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like or label indicator matrix\n",
    "        Ground truth (correct) labels for n_samples samples.\n",
    "\n",
    "    y_pred : array-like of float, shape = (n_samples, n_classes) or (n_samples,)\n",
    "        Predicted probabilities, as returned by a classifier's\n",
    "        predict_proba method. If ``y_pred.shape = (n_samples,)``\n",
    "        the probabilities provided are assumed to be that of the\n",
    "        positive class. The labels in ``y_pred`` are assumed to be\n",
    "        ordered alphabetically, as done by\n",
    "        :class:`preprocessing.LabelBinarizer`.\n",
    "\n",
    "    eps : float or \"auto\", default=\"auto\"\n",
    "        Log loss is undefined for p=0 or p=1, so probabilities are\n",
    "        clipped to `max(eps, min(1 - eps, p))`. The default will depend on the\n",
    "        data type of `y_pred` and is set to `np.finfo(y_pred.dtype).eps`.\n",
    "\n",
    "        .. versionadded:: 1.2\n",
    "\n",
    "        .. versionchanged:: 1.2\n",
    "           The default value changed from `1e-15` to `\"auto\"` that is\n",
    "           equivalent to `np.finfo(y_pred.dtype).eps`.\n",
    "\n",
    "    normalize : bool, default=True\n",
    "        If true, return the mean loss per sample.\n",
    "        Otherwise, return the sum of the per-sample losses.\n",
    "\n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Sample weights.\n",
    "\n",
    "    labels : array-like, default=None\n",
    "        If not provided, labels will be inferred from y_true. If ``labels``\n",
    "        is ``None`` and ``y_pred`` has shape (n_samples,) the labels are\n",
    "        assumed to be binary and are inferred from ``y_true``.\n",
    "\n",
    "        .. versionadded:: 0.18\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        Log loss, aka logistic loss or cross-entropy loss.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The logarithm used is the natural logarithm (base-e).\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer,\n",
    "    p. 209.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.metrics import log_loss\n",
    "    >>> log_loss([\"spam\", \"ham\", \"ham\", \"spam\"],\n",
    "    ...          [[.1, .9], [.9, .1], [.8, .2], [.35, .65]])\n",
    "    0.21616...\n",
    "    \"\"\"\n",
    "    \n",
    "    def _weighted_sum(sample_score, sample_weight, normalize=False):\n",
    "        if normalize:\n",
    "            return np.average(sample_score, weights=sample_weight)\n",
    "        elif sample_weight is not None:\n",
    "            return np.dot(sample_score, sample_weight)\n",
    "        else:\n",
    "            return sample_score.sum()    \n",
    "    y_pred = sklearn.utils.check_array(\n",
    "        y_pred, ensure_2d=False, dtype=[np.float64, np.float32, np.float16]\n",
    "    )\n",
    "    eps = np.finfo(y_pred.dtype).eps if eps == \"auto\" else eps\n",
    "\n",
    "    sklearn.utils.check_consistent_length(y_pred, y_true, sample_weight)\n",
    "    lb = sklearn.preprocessing.LabelBinarizer()\n",
    "    if labels is not None:\n",
    "        lb.fit(labels)\n",
    "    else:\n",
    "        lb.fit(y_true)\n",
    "\n",
    "    if len(lb.classes_) == 1:\n",
    "        if labels is None:\n",
    "            raise ValueError(\n",
    "                \"y_true contains only one label ({0}). Please \"\n",
    "                \"provide the true labels explicitly through the \"\n",
    "                \"labels argument.\".format(lb.classes_[0])\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The labels array needs to contain at least two \"\n",
    "                \"labels for log_loss, \"\n",
    "                \"got {0}.\".format(lb.classes_)\n",
    "            )\n",
    "\n",
    "    transformed_labels = lb.transform(y_true)\n",
    "\n",
    "    if transformed_labels.shape[1] == 1:\n",
    "        transformed_labels = np.append(\n",
    "            1 - transformed_labels, transformed_labels, axis=1\n",
    "        )\n",
    "\n",
    "    # Clipping\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # If y_pred is of single dimension, assume y_true to be binary\n",
    "    # and then check.\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred[:, np.newaxis]\n",
    "    if y_pred.shape[1] == 1:\n",
    "        y_pred = np.append(1 - y_pred, y_pred, axis=1)\n",
    "\n",
    "    # Check if dimensions are consistent.\n",
    "    transformed_labels = sklearn.utils.check_array(transformed_labels)\n",
    "    if len(lb.classes_) != y_pred.shape[1]:\n",
    "        if labels is None:\n",
    "            raise ValueError(\n",
    "                \"y_true and y_pred contain different number of \"\n",
    "                \"classes {0}, {1}. Please provide the true \"\n",
    "                \"labels explicitly through the labels argument. \"\n",
    "                \"Classes found in \"\n",
    "                \"y_true: {2}\".format(\n",
    "                    transformed_labels.shape[1], y_pred.shape[1], lb.classes_\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The number of classes in labels is different \"\n",
    "                \"from that in y_pred. Classes found in \"\n",
    "                \"labels: {0}\".format(lb.classes_)\n",
    "            )\n",
    "\n",
    "    # Renormalize\n",
    "#     print(y_pred)\n",
    "    y_pred_sum = y_pred.sum(axis=1)\n",
    "    y_pred = y_pred / y_pred_sum[:, np.newaxis]\n",
    "#     print(y_pred)\n",
    "#     print(-xlogy(transformed_labels, y_pred))\n",
    "    loss = -xlogy(transformed_labels, y_pred).sum(axis=1)\n",
    "#     print(-xlogy(transformed_labels, y_pred))\n",
    "#     print(loss)\n",
    "    return loss\n",
    "#     return _weighted_sum(loss, sample_weight, normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a677ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_matrix(embeddings,\n",
    "                      prompt_info,\n",
    "                     idx):\n",
    "\n",
    "    concept_similarity_all=[]\n",
    "    for concept_name in prompt_info.keys():\n",
    "        concept_similarity=calculate_similaity_score(\n",
    "            image_features_norm=embeddings,\n",
    "            prompt_target_embedding_norm=prompt_info[concept_name][\"prompt_target_embedding_norm\"],\n",
    "            prompt_ref_embedding_norm=prompt_info[concept_name][\"prompt_ref_embedding_norm\"],\n",
    "            temp=1/np.exp(4.5944),\n",
    "            normalize=True)\n",
    "        concept_similarity_all.append(pd.Series(concept_similarity, \n",
    "                                                index=idx,\n",
    "                                               name=concept_name\n",
    "                                               )\n",
    "                                     )\n",
    "                                      \n",
    "#                                       {\"concept_name\":concept_name,\n",
    "#                                       \"concept_similarity\":,\n",
    "#                                       })\n",
    "    concept_similarity_all=pd.concat(concept_similarity_all, axis=1)\n",
    "    \n",
    "    return concept_similarity_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0cfba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset_name in [\"clinical_fd_clean_nodup\", \"isic\"]:\n",
    "    if dataset_name==\"clinical_fd_clean_nodup\":\n",
    "        variable_dict[dataset_name].update(\n",
    "            {\"similarity_matrix_vanilla\": similarity_matrix(embeddings=variable_dict[dataset_name][\"image_features_vanilla_norm\"],\n",
    "                             prompt_info=variable_dict[dataset_name][\"prompt_info_vanilla\"],\n",
    "                            idx=variable_dict[dataset_name][\"metadata_all\"].index\n",
    "                     )})  \n",
    "    variable_dict[dataset_name].update(\n",
    "        {\"similarity_matrix\": similarity_matrix(embeddings=variable_dict[dataset_name][\"image_features_norm\"],\n",
    "                         prompt_info=variable_dict[dataset_name][\"prompt_info\"],\n",
    "                        idx=variable_dict[dataset_name][\"metadata_all\"].index\n",
    "                 )})  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47ca756",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_concept_name(dataset_name, concept_name):\n",
    "    if dataset_name==\"isic\":\n",
    "        if concept_name.startswith(\"disease\"):\n",
    "            return False\n",
    "        elif concept_name in [\"melanoma\", \"malignant\"]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        raise NotImplemented(dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bd0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "from sklearn.model_selection import train_test_split\n",
    "from MONET.datamodules.components.base_dataset import BaseDataset\n",
    "\n",
    "def get_training_data_idx(dataloader, valid_idx, y_pos, subset_idx_train, subset_idx_test, n_px=None):\n",
    "    metadata_all_new = dataloader.dataset.metadata_all.copy()\n",
    "    metadata_all_new[\"label\"]=y_pos.astype(int)\n",
    "    \n",
    "    metadata_all_new_=metadata_all_new[valid_idx]\n",
    "    \n",
    "#     print(subset_idx_train)\n",
    "    \n",
    "    train_idx, val_idx = train_test_split(np.unique(subset_idx_train), test_size=0.2, random_state=42)\n",
    "    \n",
    "    metadata_all_new_train=metadata_all_new_.loc[[i for i in subset_idx_train if i in train_idx]]\n",
    "    metadata_all_new_val=metadata_all_new_.loc[[i for i in subset_idx_train if i in val_idx]]\n",
    "    metadata_all_new_test=metadata_all_new_.loc[subset_idx_test]\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"train:\", len(metadata_all_new_train))\n",
    "    print(\"val:\", len(metadata_all_new_val))\n",
    "    print(\"test:\", len(metadata_all_new_test))\n",
    "\n",
    "    if n_px is None:\n",
    "        n_px=dataloader.dataset.n_px\n",
    "    \n",
    "    data_train = BaseDataset(\n",
    "        image_path_or_binary_dict=dataloader.dataset.image_path_dict,\n",
    "        n_px=n_px,\n",
    "        norm_mean=dataloader.dataset.transforms_aftertensor.transforms[1].mean,\n",
    "        norm_std=dataloader.dataset.transforms_aftertensor.transforms[1].std,\n",
    "        augment=False,\n",
    "        metadata_all=metadata_all_new_train,\n",
    "        integrity_level=\"weak\",\n",
    "        return_label=[\"label\"],\n",
    "    )\n",
    "\n",
    "    data_val = BaseDataset(\n",
    "        image_path_or_binary_dict=dataloader.dataset.image_path_dict,\n",
    "        n_px=n_px,\n",
    "        norm_mean=dataloader.dataset.transforms_aftertensor.transforms[1].mean,\n",
    "        norm_std=dataloader.dataset.transforms_aftertensor.transforms[1].std,\n",
    "        augment=False,\n",
    "        metadata_all=metadata_all_new_val,\n",
    "        integrity_level=\"weak\",\n",
    "        return_label=[\"label\"],\n",
    "    )\n",
    "    \n",
    "    data_test = BaseDataset(\n",
    "        image_path_or_binary_dict=dataloader.dataset.image_path_dict,\n",
    "        n_px=n_px,\n",
    "        norm_mean=dataloader.dataset.transforms_aftertensor.transforms[1].mean,\n",
    "        norm_std=dataloader.dataset.transforms_aftertensor.transforms[1].std,\n",
    "        augment=False,\n",
    "        metadata_all=metadata_all_new_test,\n",
    "        integrity_level=\"weak\",\n",
    "        return_label=[\"label\"],\n",
    "    )    \n",
    "\n",
    "    from MONET.utils.loader import custom_collate\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=data_train,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "#         pin_memory=True,\n",
    "#         persistent_workers=False,\n",
    "        shuffle=True,\n",
    "        collate_fn=custom_collate,\n",
    "    )\n",
    "    val_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=data_val,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "#         pin_memory=True,\n",
    "#         persistent_workers=False,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate,\n",
    "    )   \n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        dataset=data_test,\n",
    "        batch_size=64,\n",
    "        num_workers=4,\n",
    "#         pin_memory=True,\n",
    "#         persistent_workers=False,\n",
    "        shuffle=False,\n",
    "        collate_fn=custom_collate,\n",
    "    )       \n",
    "    \n",
    "    return train_dataloader, val_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c5939",
   "metadata": {},
   "source": [
    "# forced direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab05e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name=\"clinical_fd_clean_nodup\"\n",
    "\n",
    "simulation_data_list=[]\n",
    "\n",
    "for concept_name in variable_dict[dataset_name][\"concept_list\"]:\n",
    "    skincon_idx=(variable_dict[dataset_name][\"metadata_all\"][\"skincon_Do not consider this image\"]==0).values\n",
    "    if variable_dict[dataset_name][\"metadata_all\"][concept_name][skincon_idx].values.astype(bool).sum()<30:\n",
    "        print(concept_name, \"!!!!!!!!!!!!!!!!!!!!!!!! SKIPPED !!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        continue  \n",
    "    \n",
    "#     if concept_name!=\"skincon_Brown(Hyperpigmentation)\":\n",
    "#         continue\n",
    "    \n",
    "    \n",
    "    num_train_pos=500\n",
    "    num_train_neg=500\n",
    "    \n",
    "    num_test_pos=500\n",
    "    num_test_neg=500\n",
    "#     for proportion in [1.0, 0.8, 0.2, 0.0]:\n",
    "    for proportion in [1]:\n",
    "        num_train_pos_with = int(num_train_pos*proportion)\n",
    "        num_train_pos_without = num_train_pos-num_train_pos_with\n",
    "        \n",
    "        num_train_neg_with = int(num_train_neg*(1-proportion))\n",
    "        num_train_neg_without = num_train_neg-num_train_neg_with\n",
    "        \n",
    "        \n",
    "        num_test_pos_with = int(num_test_pos*(1-proportion))\n",
    "        num_test_pos_without = num_test_pos-num_test_pos_with\n",
    "        \n",
    "        num_test_neg_with = int(num_test_neg*(proportion))\n",
    "        num_test_neg_without = num_test_neg-num_test_neg_with\n",
    "        \n",
    "        \n",
    "        for random_seed in range(20):\n",
    "            subset_idx_train_, subset_idx_test_ = train_test_split(np.arange(len(skincon_idx))[skincon_idx], \n",
    "                                                                   test_size=0.4, \n",
    "                                                                   random_state=random_seed)\n",
    "\n",
    "\n",
    "\n",
    "            metadata_all_train_=variable_dict[dataset_name][\"metadata_all\"].iloc[subset_idx_train_]\n",
    "            y_pos_train_=variable_dict[dataset_name][\"y_pos\"][subset_idx_train_]\n",
    "            metadata_all_train_=metadata_all_train_.copy()\n",
    "            metadata_all_train_[\"y_pos\"]=y_pos_train_\n",
    "            \n",
    "            if len(metadata_all_train_[(metadata_all_train_[concept_name]==1)&(metadata_all_train_[\"y_pos\"]==True)])<30 or\\\n",
    "            len(metadata_all_train_[(metadata_all_train_[concept_name]==0)&(metadata_all_train_[\"y_pos\"]==True)])<30 or\\\n",
    "            len(metadata_all_train_[(metadata_all_train_[concept_name]==1)&(metadata_all_train_[\"y_pos\"]==False)])<30 or\\\n",
    "            len(metadata_all_train_[(metadata_all_train_[concept_name]==0)&(metadata_all_train_[\"y_pos\"]==False)])<30:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            train_idx_pos_with=metadata_all_train_[(metadata_all_train_[concept_name]==1)&(metadata_all_train_[\"y_pos\"]==True)].sample(n=num_train_pos_with, replace=True, random_state=random_seed).index\n",
    "            train_idx_pos_without=metadata_all_train_[(metadata_all_train_[concept_name]==0)&(metadata_all_train_[\"y_pos\"]==True)].sample(n=num_train_pos_without, replace=True, random_state=random_seed).index\n",
    "            \n",
    "            train_idx_neg_with=metadata_all_train_[(metadata_all_train_[concept_name]==1)&(metadata_all_train_[\"y_pos\"]==False)].sample(n=num_train_neg_with, replace=True, random_state=random_seed).index\n",
    "            train_idx_neg_without=metadata_all_train_[(metadata_all_train_[concept_name]==0)&(metadata_all_train_[\"y_pos\"]==False)].sample(n=num_train_neg_without, replace=True, random_state=random_seed).index\n",
    "            \n",
    "            train_idx=train_idx_pos_with.tolist()+train_idx_pos_without.tolist()+train_idx_neg_with.tolist()+train_idx_neg_without.tolist()\n",
    "            # train_idx=metadata_all_train_[(metadata_all_train_[concept_name]==1] ###\n",
    "            #train_idx=metadata_all_train_.index.tolist()\n",
    "            #print(metadata_all_train_.loc[train_idx][[concept_name,\"y_pos\"]])\n",
    "            metadata_all_train=metadata_all_train_.loc[train_idx]\n",
    "            \n",
    "            \n",
    "            metadata_all_test_=variable_dict[dataset_name][\"metadata_all\"].iloc[subset_idx_test_]\n",
    "            y_pos_test_=variable_dict[dataset_name][\"y_pos\"][subset_idx_test_]\n",
    "            metadata_all_test_=metadata_all_test_.copy()\n",
    "            metadata_all_test_[\"y_pos\"]=y_pos_test_\n",
    "            \n",
    "            if len(metadata_all_test_[(metadata_all_test_[concept_name]==1)&(metadata_all_test_[\"y_pos\"]==True)])<30 or\\\n",
    "            len(metadata_all_test_[(metadata_all_test_[concept_name]==0)&(metadata_all_test_[\"y_pos\"]==True)])<30 or\\\n",
    "            len(metadata_all_test_[(metadata_all_test_[concept_name]==1)&(metadata_all_test_[\"y_pos\"]==False)])<30 or\\\n",
    "            len(metadata_all_test_[(metadata_all_test_[concept_name]==0)&(metadata_all_test_[\"y_pos\"]==False)])<30:        \n",
    "                continue\n",
    "            \n",
    "            \n",
    "            test_idx_pos_with=metadata_all_test_[(metadata_all_test_[concept_name]==1)&(metadata_all_test_[\"y_pos\"]==True)].sample(n=num_test_pos_with, replace=True, random_state=random_seed).index\n",
    "            test_idx_pos_without=metadata_all_test_[(metadata_all_test_[concept_name]==0)&(metadata_all_test_[\"y_pos\"]==True)].sample(n=num_test_pos_without, replace=True, random_state=random_seed).index\n",
    "            \n",
    "            test_idx_neg_with=metadata_all_test_[(metadata_all_test_[concept_name]==1)&(metadata_all_test_[\"y_pos\"]==False)].sample(n=num_test_neg_with, replace=True, random_state=random_seed).index\n",
    "            test_idx_neg_without=metadata_all_test_[(metadata_all_test_[concept_name]==0)&(metadata_all_test_[\"y_pos\"]==False)].sample(n=num_test_neg_without, replace=True, random_state=random_seed).index\n",
    "            \n",
    "            test_idx=test_idx_pos_with.tolist()+test_idx_pos_without.tolist()+test_idx_neg_with.tolist()+test_idx_neg_without.tolist()\n",
    "            \n",
    "            \n",
    "            metadata_all_test=metadata_all_test_.loc[test_idx]\n",
    "            print(len(train_idx), len(test_idx))\n",
    "            \n",
    "            train_dataloader, val_dataloader, test_dataloader=\\\n",
    "            get_training_data_idx(dataloader=variable_dict[dataset_name][\"dataloader\"], \n",
    "                              valid_idx=skincon_idx, \n",
    "                              y_pos=variable_dict[dataset_name][\"y_pos\"], \n",
    "#                               y_pos=variable_dict[dataset_name][\"dataloader\"].dataset.metadata_all[concept_name].fillna(0), \n",
    "                              subset_idx_train=metadata_all_train.index, \n",
    "                              #subset_idx_test=test_idx, \n",
    "                              subset_idx_test=metadata_all_test.index,\n",
    "                              n_px=None)            \n",
    "    \n",
    "#             print(len(train_dataloader))\n",
    "#             print(len(val_dataloader))\n",
    "#             print(len(test_dataloader))\n",
    "#             subset_idx_train=\n",
    "            \n",
    "            auc, x, logits_test, label_test, metadata_test, max_f1_thres =train_classifier(train_dataloader=train_dataloader, \n",
    "                                   val_dataloader=val_dataloader,\n",
    "                                   test_dataloader=test_dataloader, verbose=True)  \n",
    "        \n",
    "            metadata_test=pd.concat(metadata_test)  \n",
    "            label_test=pd.Series(np.hstack(label_test), index=metadata_test.index)\n",
    "            logit_test=pd.Series(np.concatenate(logits_test)[:,0], index=metadata_test.index)\n",
    "            \n",
    "            simulation_data_list.append({\"concept_name\": concept_name,\n",
    "                                         \"random_seed\": random_seed,\n",
    "                                         \"proportion\": proportion,\n",
    "                                         \"label_test\": label_test,\n",
    "                                         \"logit_test\": logit_test,\n",
    "                                         \"metadata_all_train\": metadata_all_train,\n",
    "                                         \"metadata_all_test\": metadata_all_test,\n",
    "                                         \"metadata_test\": metadata_test,\n",
    "                                         \"max_f1_thres\": max_f1_thres,\n",
    "                                        })\n",
    "#             label_list=np.hstack(label_list)\n",
    "#             logits_list=np.concatenate(logits_list)[:,0]\n",
    "#             metadata_list=pd.concat(metadata_list)         \n",
    "\n",
    "#         record_dict_list.append({\"concept_name\": concept_name})\n",
    "        print(concept_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c345a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd626a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9da2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc498a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e4ba34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2988d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ff29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(simulation_data_list, \"logs/experiment_results/model_audit_benchmark_0525.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e8314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulation_data_list=torch.load(\"logs/experiment_results/model_audit_benchmark_0525.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa7a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_data_list=torch.load(\"logs/experiment_results/model_audit_benchmark_0526.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ea24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(variable_dict, \"logs/experiment_results/model_audit_data_0526.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76138722",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_dict=torch.load(\"logs/experiment_results/model_audit_data_0526.pt\", map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0c43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(simulation_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413a0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit_test\n",
    "\n",
    "# log_loss(y_pred=logit_test,\n",
    "#         y_true=label_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81d0613",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50830452",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_dict[dataset_name].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25688725",
   "metadata": {},
   "source": [
    "current method+CLIP\n",
    "DOMINO+MONET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1f96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_info_copy=cluster_concept_test(similarity_info=concept_similarity_all,\n",
    "#                                    ground_truth=variable_dict[\"clinical_fd_clean_nodup\"][\"metadata_all\"][skincon_cols],\n",
    "#                                    clustering_features=concept_similarity_all,\n",
    "#                                    labels=label_test, logits=logit_test,\n",
    "#                                    threshold=0,\n",
    "#                                    score_threshold=0.8, accuracy_diff=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3bb2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import fisher_exact\n",
    "def fisher_test_df(data, columns, y_pos):\n",
    "    res_df=[]\n",
    "    for column in columns:\n",
    "#         print(column)\n",
    "#         print([[((data[column]==1)&(y_pos.loc[data.index]==True)).sum(), ((data[column]==0)&(y_pos.loc[data.index]==True)).sum()],\n",
    "#             [((data[column]==1)&(y_pos.loc[data.index]==False)).sum(), ((data[column]==0)&(y_pos.loc[data.index]==False)).sum()]])\n",
    "#         print(data.shape)\n",
    "#         print(((data[column]==1).shape,(y_pos.loc[data.index]==True).shape))\n",
    "\n",
    "        y_1_c_1=((y_pos==True)&(data[column]==1)).sum()\n",
    "        y_1_c_0=((y_pos==True)&(data[column]==0)).sum()\n",
    "        y_0_c_1=((y_pos==False)&(data[column]==1)).sum()\n",
    "        y_0_c_0=((y_pos==False)&(data[column]==0)).sum()\n",
    "    \n",
    "\n",
    "        res=fisher_exact(\n",
    "            [[y_1_c_1, y_1_c_0],\n",
    "            [y_0_c_1, y_0_c_0]])\n",
    "    \n",
    "#         rl_top=((data[column]==1)&(y_pos==True)).sum()/(((data[column]==1)).sum())\n",
    "#         rl_bottom=((data[column]==0)&(y_pos==True)).sum()/(((data[column]==0)).sum())\n",
    "        \n",
    "#         rl_top=((data[column]==1)&(y_pos==True)).sum()/(((y_pos==True)).sum())\n",
    "#         rl_bottom=((data[column]==1)&(y_pos==False)).sum()/(((y_pos==False)).sum())        \n",
    "#         print(res)\n",
    "        direction=(y_1_c_1-y_1_c_0)*(y_0_c_1-y_0_c_0)\n",
    "        \n",
    "        res_df.append({\"name\": column,\n",
    "                       \"y=1,c=1\":y_1_c_1,\n",
    "                       \"y=1,c=0\":y_1_c_0,\n",
    "                       \"y=0,c=1\":y_0_c_1,\n",
    "                       \"y=0,c=0\":y_0_c_0,\n",
    "                       \"direction\":direction,\n",
    "                       \"direction1\":(y_1_c_1-y_1_c_0),\n",
    "                       \"direction2\":(y_0_c_1-y_0_c_0),                       \n",
    "#                        \"rl\": rl_top/ rl_bottom,\n",
    "                      \"pvalue\": res.pvalue,\n",
    "                       \"statistic\": res.statistic,\n",
    "                      })\n",
    "        \n",
    "#         print()\n",
    "    \n",
    "    res_df=pd.DataFrame(res_df).sort_values('statistic').set_index('name')\n",
    "    fdr_corrected=fdrcorrection(res_df[\"pvalue\"])\n",
    "    res_df[\"FDR_rejected\"]=fdr_corrected[0]\n",
    "    res_df[\"FDR_pvalue_adjusted\"]=fdr_corrected[1]    \n",
    "    res_df[\"bof_pvalue_adjusted\"]=res_df[\"pvalue\"]*len(res_df)\n",
    "    res_df[\"bof_pvalue_adjusted\"]=res_df[\"bof_pvalue_adjusted\"].map(lambda x:1 if x>1 else x)\n",
    "    #res_df[\"bof_rejected\"]=res_df[\"bof_pvalue_adjusted\"]<0.05\n",
    "    res_df[\"bof_rejected\"]=res_df[\"bof_pvalue_adjusted\"]<0.01\n",
    "    return res_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e08bc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6597cc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00947624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cb524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "from scipy.stats import mode\n",
    "\n",
    "def cluster_concept_test_real(similarity_info, clustering_features, fixed_answer,\n",
    "                         labels, logits, threshold,\n",
    "                         metric_diff=0.5,\n",
    "                         n_clusters=40, random_state=42):\n",
    "    \n",
    "    torch.manual_seed(random_state)\n",
    "    random.seed(random_state)\n",
    "    np.random.seed(random_state)    \n",
    "    \n",
    "    record_list=[]\n",
    "    \n",
    "    per_label=False\n",
    "    \n",
    "    if per_label:\n",
    "        labels_unique=np.unique(labels)\n",
    "    else:\n",
    "        labels_unique=[None]\n",
    "        \n",
    "    for label in labels_unique:\n",
    "        if label is not None:\n",
    "            focus_idx=labels[labels==label].index\n",
    "\n",
    "            similarity_info_focus=similarity_info.loc[focus_idx].copy()\n",
    "            clustering_features_focus=clustering_features.loc[focus_idx].copy()\n",
    "            labels_focus=labels[labels==label].copy()\n",
    "            logits_focus=logits[labels==label].copy()\n",
    "        else:\n",
    "            focus_idx=labels[labels.astype(int)>-9].index\n",
    "\n",
    "            similarity_info_focus=similarity_info.loc[focus_idx].copy()\n",
    "            clustering_features_focus=clustering_features.loc[focus_idx].copy()\n",
    "            labels_focus=labels.copy()\n",
    "            logits_focus=logits.copy()            \n",
    "            \n",
    "            \n",
    "        assert (similarity_info_focus.index==clustering_features_focus.index).all()\n",
    "        assert (similarity_info_focus.index==labels_focus.index).all()\n",
    "        assert (similarity_info_focus.index==logits_focus.index).all()\n",
    "\n",
    "        if clustering_features_focus.shape[1]<50:\n",
    "            pca = PCA(n_components=10)\n",
    "        else:\n",
    "            pca = PCA(n_components=50)\n",
    "\n",
    "        clustering_features_focus_pc=pca.fit_transform(clustering_features_focus)\n",
    "    \n",
    "        kmeans = KMeans(n_clusters=n_clusters//len(labels_unique), random_state=random_state, n_init=\"auto\").fit(clustering_features_focus_pc)\n",
    "        kmeans_dist=sklearn.metrics.pairwise_distances(kmeans.cluster_centers_)\n",
    "    \n",
    "        similarity_info_focus_copy=similarity_info_focus.copy()\n",
    "        similarity_info_focus_copy[\"kmeans_label\"]=kmeans.labels_\n",
    "        similarity_info_focus_copy[\"kmeans_dist\"]=((clustering_features_focus_pc-kmeans.cluster_centers_[kmeans.labels_])**2).sum(axis=1)\n",
    "        similarity_info_focus_copy[\"accuracy\"]=(labels_focus==(logits_focus>threshold))\n",
    "        similarity_info_focus_copy[\"loss\"]=-log_loss(y_true=labels_focus, y_pred=logits_focus.map(lambda x: 1/(1+np.exp(-x))), labels=[0,1])\n",
    "        similarity_info_focus_copy[\"label\"]=labels_focus\n",
    "        similarity_info_focus_copy[\"logit\"]=logits_focus\n",
    "        \n",
    "        similarity_info_focus_copy_group=similarity_info_focus_copy.groupby(\"kmeans_label\")[similarity_info.columns.tolist()].apply(lambda x: pd.Series([x[i].values for i in x.columns], index=x.columns))\n",
    "        similarity_info_focus_copy_group[\"count\"]=similarity_info_focus_copy.groupby(\"kmeans_label\").apply(len)\n",
    "        similarity_info_focus_copy_group[\"accuracy\"]=similarity_info_focus_copy.groupby(\"kmeans_label\")[\"accuracy\"].mean()\n",
    "        similarity_info_focus_copy_group[\"loss\"]=similarity_info_focus_copy.groupby(\"kmeans_label\")[\"loss\"].mean()\n",
    "        similarity_info_focus_copy_group[\"label_frequent\"]=similarity_info_focus_copy.groupby(\"kmeans_label\")[\"label\"].apply(lambda x: mode(x, keepdims=False).mode)\n",
    "\n",
    "        metric_use=\"accuracy\"\n",
    "\n",
    "        for count, (idx, row) in enumerate(similarity_info_focus_copy_group.sort_values(metric_use, ascending=True).iterrows()):\n",
    "            if row[metric_use]>=similarity_info_focus_copy[metric_use].mean():\n",
    "                continue\n",
    "\n",
    "            sorted_idx=pd.Series(kmeans_dist[idx], index=sorted(np.unique(kmeans.labels_))).sort_values(ascending=True).index\n",
    "            sorted_idx=[i for i in sorted_idx if similarity_info_focus_copy_group.loc[i][metric_use]>(similarity_info_focus_copy[metric_use].mean()+metric_diff)]\n",
    "            \n",
    "            similarity_info_focus_copy_group_diff_plus=similarity_info_focus_copy_group.copy().loc[[sorted_idx[0]]]\n",
    "#             print(similarity_info_focus_copy_group_diff_plus)\n",
    "            similarity_info_focus_copy_group_diff_plus[similarity_info.columns.tolist()]=similarity_info_focus_copy_group_diff_plus[similarity_info.columns.tolist()].apply(lambda x: pd.Series([(np.mean(row[i])-np.mean(x.loc[i])) for i in x.index], index=x.index), axis=1)\n",
    "    \n",
    "            x=pd.concat([\n",
    "                similarity_info_focus_copy_group_diff_plus[similarity_info.columns.tolist()].loc[sorted_idx[0]].rename('diff_magnitude'),\n",
    "                row[similarity_info.columns.tolist()].map(lambda x: np.mean(x)).rename(\"mean_value\")\n",
    "            ],\n",
    "                axis=1)\n",
    "#             print(x.sort_values(\"diff_magnitude\", ascending=False))    \n",
    "                        \n",
    "            similarity_info_focus_copy_group_diff_minus=similarity_info_focus_copy_group.copy().loc[[sorted_idx[0]]]\n",
    "            similarity_info_focus_copy_group_diff_minus[similarity_info.columns.tolist()]=similarity_info_focus_copy_group_diff_minus[similarity_info.columns.tolist()].apply(lambda x: pd.Series([-(np.mean(row[i])-np.mean(x.loc[i])) for i in x.index], index=x.index), axis=1)            \n",
    "            \n",
    "            \n",
    "            \n",
    "#             print('-------')\n",
    "#             print(x.sort_values('diff_magnitude', ascending=True).index==similarity_info_focus_copy_group_diff_minus[similarity_info.columns.tolist()].loc[sorted_idx[0]].sort_values(ascending=False).index.tolist())\n",
    "#             print(x.sort_values('diff_magnitude', ascending=True))\n",
    "#             print(idx, sorted_idx)\n",
    "            \n",
    "            \n",
    "            \n",
    "            record_list.append(\n",
    "                { \n",
    "#                  \"on_the_spot_plus_pred\": similarity_info_focus_copy_group_diff_plus[similarity_info.columns.tolist()].loc[sorted_idx[0]].sort_values(ascending=False).index.tolist(),\n",
    "#                  \"on_the_spot_minus_pred\": similarity_info_focus_copy_group_diff_minus[similarity_info.columns.tolist()].loc[sorted_idx[0]].sort_values(ascending=False).index.tolist(),\n",
    "                 \"on_the_spot_plus_pred\": x[(x[\"mean_value\"]>0.5)&(x[\"diff_magnitude\"]>0)].sort_values(\"diff_magnitude\", ascending=False).index.tolist(),\n",
    "                 \"on_the_spot_minus_pred\": x[(x[\"mean_value\"]>0.5)&(x[\"diff_magnitude\"]>0)].sort_values(\"diff_magnitude\", ascending=True).index.tolist(),                    \n",
    "                 \"statistics\": x,\n",
    "                 \"labels\": similarity_info_focus_copy[(similarity_info_focus_copy[\"kmeans_label\"]==idx)][[\"kmeans_dist\", metric_use]],\n",
    "                 \"labels_ref\": similarity_info_focus_copy[(similarity_info_focus_copy[\"kmeans_label\"]==sorted_idx[0])][[\"kmeans_dist\", metric_use]]                 \n",
    "                })\n",
    "#             print(record_list[-1][\"statistics\"].sort_values(\"diff_magnitude\", ascending=False).index==record_list[-1][\"on_the_spot_minus_pred\"]).all()\n",
    "    \n",
    "    return record_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a670da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate_model_audit(simulation_data_list):\n",
    "    \n",
    "    def get_ground_truth_on_the_spot(ground_truth, target_idx, reference_idx):\n",
    "        ground_truth_copy=ground_truth.copy()\n",
    "        \n",
    "        ground_truth_copy_target=ground_truth_copy.loc[target_idx]\n",
    "        ground_truth_copy_reference=ground_truth_copy.loc[reference_idx]\n",
    "    \n",
    "    \n",
    "        ground_truth_copy_target=(ground_truth_copy_target.mean(axis=0)>0.5).astype(int)\n",
    "        ground_truth_copy_reference=(ground_truth_copy_reference.mean(axis=0)>0.5).astype(int)\n",
    "        ground_truth_copy_target_diff=ground_truth_copy_target-ground_truth_copy_reference\n",
    "        return {\"more_present\":ground_truth_copy_target_diff[ground_truth_copy_target_diff>0].index,\n",
    "                \"less_present\":ground_truth_copy_target_diff[ground_truth_copy_target_diff<0].index}\n",
    "                \n",
    "                \n",
    "    record_dict_all=[]\n",
    "    for simulation_count, simulation_data in enumerate(tqdm.tqdm(simulation_data_list)):\n",
    "#         if simulation_count not in [0,20,40,60,80]:\n",
    "#             continue\n",
    "        \n",
    "        concept_name=simulation_data[\"concept_name\"]\n",
    "        label_test=simulation_data[\"label_test\"]\n",
    "        logit_test=simulation_data[\"logit_test\"]\n",
    "        metadata_train=simulation_data[\"metadata_all_train\"]        \n",
    "        metadata_test=simulation_data[\"metadata_all_test\"]                \n",
    "#         metadata_test=simulation_data[\"metadata_test\"]\n",
    "        max_f1_thres=simulation_data[\"max_f1_thres\"]\n",
    "        \n",
    "        \n",
    "        fisher_pvals_train=fisher_test_df(data=metadata_train,\n",
    "                   columns=variable_dict[\"clinical_fd_clean_nodup\"][\"concept_list\"],\n",
    "                   y_pos=metadata_train[\"y_pos\"])\n",
    "        \n",
    "\n",
    "        fisher_pvals_test=fisher_test_df(data=metadata_test,\n",
    "                       columns=variable_dict[\"clinical_fd_clean_nodup\"][\"concept_list\"],\n",
    "                       y_pos=metadata_test[\"y_pos\"])        \n",
    "        \n",
    "#         print(fisher_pvals_train)\n",
    "#         print(fisher_pvals_test)\n",
    "        \n",
    "        \n",
    "        test_less_represented=fisher_pvals_train[(fisher_pvals_train[\"direction\"]<0)&(fisher_pvals_train[\"statistic\"]>1)].index\\\n",
    "        .intersection(fisher_pvals_test[(fisher_pvals_test[\"direction\"]<0)&(fisher_pvals_test[\"statistic\"]<1)].index)\n",
    "        \n",
    "        test_more_represented=fisher_pvals_train[(fisher_pvals_train[\"direction\"]<0)&(fisher_pvals_train[\"statistic\"]<1)].index\\\n",
    "        .intersection(fisher_pvals_test[(fisher_pvals_test[\"direction\"]<0)&(fisher_pvals_test[\"statistic\"]>1)].index)\n",
    "        \n",
    "        test_more_represented_=fisher_pvals_train[(fisher_pvals_train[\"direction1\"]>0)&(fisher_pvals_train[\"direction2\"]<0)].index\\\n",
    "        .intersection(fisher_pvals_test[(fisher_pvals_test[\"direction1\"]<0)&(fisher_pvals_test[\"direction2\"]>0)].index)\n",
    "        \n",
    "        test_less_represented_=fisher_pvals_train[(fisher_pvals_train[\"direction1\"]<0)&(fisher_pvals_train[\"direction2\"]>0)].index\\\n",
    "        .intersection(fisher_pvals_test[(fisher_pvals_test[\"direction1\"]>0)&(fisher_pvals_test[\"direction2\"]<0)].index)        \n",
    "        \n",
    "        print(concept_name, \"Train-/Test+\", test_more_represented.tolist(), \"Train+/Test-\", test_less_represented.tolist())\n",
    "        print(\"Test\",concept_name, \"Train-/Test+\", test_more_represented_.tolist(), \"Train+/Test-\", test_less_represented_.tolist())\n",
    "                \n",
    "        fixed_answer=test_more_represented.tolist()+test_less_represented.tolist()\n",
    "\n",
    "        test_result_MONET=cluster_concept_test_real(similarity_info=variable_dict[\"clinical_fd_clean_nodup\"][\"similarity_matrix\"][skincon_cols],\n",
    "                                                    clustering_features=pd.DataFrame(variable_dict[\"clinical_fd_clean_nodup\"][\"efficientnet_feature\"].numpy(),\n",
    "                                                                     index=variable_dict[\"clinical_fd_clean_nodup\"][\"metadata_all\"].index,\n",
    "                                                                    ),\n",
    "                                                    fixed_answer=fixed_answer,\n",
    "                                                    labels=label_test, logits=logit_test,\n",
    "                                                    threshold=max_f1_thres,\n",
    "        #                                            score_threshold=0.8, \n",
    "                                                    metric_diff=0,\n",
    "                                                    n_clusters=40)\n",
    "        \n",
    "        test_result_vanilla=cluster_concept_test_real(similarity_info=variable_dict[\"clinical_fd_clean_nodup\"][\"similarity_matrix_vanilla\"][skincon_cols],\n",
    "                                                    clustering_features=pd.DataFrame(variable_dict[\"clinical_fd_clean_nodup\"][\"efficientnet_feature\"].numpy(),\n",
    "                                                                     index=variable_dict[\"clinical_fd_clean_nodup\"][\"metadata_all\"].index,\n",
    "                                                                    ),\n",
    "                                                    fixed_answer=fixed_answer,\n",
    "                                                    labels=label_test, logits=logit_test,\n",
    "                                                    threshold=max_f1_thres,\n",
    "        #                                            score_threshold=0.8, \n",
    "                                                    metric_diff=0.1,\n",
    "                                                    n_clusters=40)\n",
    "        \n",
    "        \n",
    "        for model in [\"MONET\", \"CLIP\"]:\n",
    "            if model==\"MONET\":\n",
    "                test_result_list=test_result_MONET\n",
    "            elif model==\"CLIP\":\n",
    "                test_result_list=test_result_vanilla\n",
    "            else:\n",
    "                raise\n",
    "        \n",
    "            for test_result in test_result_list:\n",
    "                ground_truth_on_the_spot=get_ground_truth_on_the_spot(ground_truth=variable_dict[\"clinical_fd_clean_nodup\"][\"metadata_all\"][skincon_cols], \n",
    "                                             target_idx=test_result[\"labels\"].index, \n",
    "                                             reference_idx=test_result[\"labels_ref\"].index)\n",
    "        \n",
    "#                 print(model, len(test_result_list))\n",
    "                for i in range(1,5+1):\n",
    "                    if i>len(test_result[\"on_the_spot_plus_pred\"]):\n",
    "                        continue\n",
    "                    record_dict_all.append({\n",
    "                        \"model\": model,\n",
    "                        \"method\": \"on_the_spot_plus\",\n",
    "                        \"rank_n\": i,\n",
    "                        \"metric\": len(set(ground_truth_on_the_spot[\"more_present\"]).intersection(test_result[\"on_the_spot_plus_pred\"][:i]))!=0,\n",
    "                        \"answer_length\": len(set(ground_truth_on_the_spot[\"more_present\"])),\n",
    "                        \"random_performance\": 1-(math.perm(48-len(set(ground_truth_on_the_spot[\"more_present\"])), i) / math.perm(48, i)),\n",
    "                    })\n",
    "\n",
    "                for i in range(1,5+1):\n",
    "                    if i>len(test_result[\"on_the_spot_minus_pred\"]):\n",
    "                        continue                 \n",
    "                    record_dict_all.append({\n",
    "                        \"model\": model,\n",
    "                        \"method\": \"on_the_spot_minus\",\n",
    "                        \"rank_n\": i,\n",
    "                        \"metric\": len(set(ground_truth_on_the_spot[\"less_present\"]).intersection(test_result[\"on_the_spot_minus_pred\"][:i]))!=0,\n",
    "                        \"answer_length\": len(set(ground_truth_on_the_spot[\"less_present\"]))\n",
    "                    })  \n",
    "                    \n",
    "#                 for i in range(1,5+1):\n",
    "#                     record_dict_all.append({\n",
    "#                         \"model\": model,\n",
    "#                         \"method\": \"on_the_spot_both\",\n",
    "#                         \"rank_n\": i,\n",
    "#                         \"metric\": len(set(ground_truth_on_the_spot[\"more_present\"]).intersection(test_result[\"on_the_spot_plus_pred\"][:i]))!=0 and len(set(ground_truth_on_the_spot[\"less_present\"]).intersection(test_result[\"on_the_spot_minus_pred\"][:i]))!=0,\n",
    "#                     })                      \n",
    "                    \n",
    "            for i in range(1,5+1):      \n",
    "                record_dict_all.append({\n",
    "                    \"model\": model,\n",
    "                    \"method\": \"fixed_answer_plus\",\n",
    "                    \"answer_length\": len(set(fixed_answer)),\n",
    "                    \"count\": len(test_result_list),\n",
    "                    \"rank_n\": i,\n",
    "                    \"metric\": len(set(fixed_answer).intersection([p for test_result in test_result_list for p in test_result[\"on_the_spot_plus_pred\"][:i]]))!=0,\n",
    "                    \"random_performance\": 1-(math.comb(48-len(set(fixed_answer)), i) / math.comb(48, i))**len(test_result_list),\n",
    "                })    \n",
    "                \n",
    "            for i in range(1,5+1):\n",
    "                record_dict_all.append({\n",
    "                    \"model\": model,\n",
    "                    \"method\": \"fixed_answer_minus\",\n",
    "                    \"answer_length\": len(set(fixed_answer)),\n",
    "                    \"count\": len(test_result_list),\n",
    "                    \"rank_n\": i,\n",
    "                    \"metric\": len(set(fixed_answer).intersection([p for test_result in test_result_list for p in test_result[\"on_the_spot_minus_pred\"][:i]]))!=0,\n",
    "                })    \n",
    "                                  \n",
    "        \n",
    "        \n",
    "    return record_dict_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3042eb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_audit(simulation_data_list=simulation_data_list[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5214503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11657e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_df[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5fd708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2539344f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad243232",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0088107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b306fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(math.comb(48-2, 1) / math.comb(48, 1))**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ed045",
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(math.comb(48-2, 2) / math.comb(48, 2))**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8ca2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(math.comb(48-2, 4) / math.comb(48, 4))**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afe222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_df[eval_result_df[\"method\"]==\"fixed_answer_plus\"].groupby([\"method\", \"rank_n\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd667e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "1-(math.perm(48-len(set(ground_truth_on_the_spot[\"more_present\"])), i) / math.perm(48, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a8258",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_df[(eval_result_df[\"method\"]==\"fixed_answer_plus\")].groupby([\"rank_n\", \"method\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164c1b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ec85e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_df.plot(x=\"random_performance\", y=\"random_performance_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36757a2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# eval_result_df[eval_result_df[\"method\"]==\"fixed_answer_minus\"][\"answer_length\"].hist()\n",
    "# eval_result_df[eval_result_df[\"method\"]==\"on_the_spot_minus\"][\"answer_length\"].hist()\n",
    "# eval_result=evaluate_model_audit(simulation_data_list=\n",
    "# [\n",
    "#     simulation_data_list[0],\n",
    "#     simulation_data_list[20],\n",
    "#     simulation_data_list[40],\n",
    "#     simulation_data_list[60],\n",
    "#     simulation_data_list[80],    \n",
    "\n",
    "# ])\n",
    "eval_result=evaluate_model_audit(simulation_data_list=simulation_data_list)\n",
    "eval_result_df=pd.DataFrame(eval_result)\n",
    "eval_result_df[\"metric_random_ratio\"]=eval_result_df[\"metric\"].astype(int)/eval_result_df[\"random_performance\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758d193",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_data_list[0][\"concept_name\"],\\\n",
    "simulation_data_list[20][\"concept_name\"],\\\n",
    "simulation_data_list[40][\"concept_name\"],\\\n",
    "simulation_data_list[60][\"concept_name\"],\\\n",
    "simulation_data_list[80][\"concept_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fdce8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# variable_dict[\"clinical_fd_clean_nodup\"][\"metadata_all\"][\n",
    "# (variable_dict[\"clinical_fd_clean_nodup\"][\"metadata_all\"][\"skincon_Crust\"]==0)\n",
    "# &(variable_dict[\"clinical_fd_clean_nodup\"][\"y_pos\"]==False)\n",
    "# ].iloc[5:]\n",
    "# variable_dict[\"clinical_fd_clean_nodup\"][\"dataloader\"].dataset.getitem(\n",
    "# variable_dict[\"clinical_fd_clean_nodup\"][\"dataloader\"].dataset.metadata_all.index.tolist().index(\n",
    "# \"7d2f3fa05f4f362299c1ed148e7fc719.jpg\")\n",
    "# )[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6103ab8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47cfacbd",
   "metadata": {},
   "source": [
    "# one ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a6eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.prop_cycle\"]=cycler('color', [np.array(i)/256 for i in [Paired[12][1], \n",
    "                                                                                Paired[12][3],\n",
    "                                                                                Paired[12][5],\n",
    "                                                                                Paired[12][7],\n",
    "                                                                                Paired[12][9],\n",
    "                                                                                Paired[12][11]\n",
    "                                                                                ]])\n",
    "# fig = plt.figure(constrained_layout=True, figsize=(15, 6))\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "subfigs = fig.subfigures(1, 1)\n",
    "\n",
    "axes = subfigs.subplots(1,2, gridspec_kw={\"wspace\":0.3})\n",
    "\n",
    "axd={'fixed': axes[0], \"on_the_spot\": axes[1] }\n",
    "\n",
    "plot_key=\"fixed\"\n",
    "\n",
    "\n",
    "# sns.barplot(x=\"rank_n\", y=\"metric\", hue=\"model\", hue_order=[\"MONET\", \"CLIP\"], \n",
    "#             data=eval_result_df[(eval_result_df[\"method\"]==\"fixed_answer_plus\")&(eval_result_df[\"rank_n\"]<=3)],\n",
    "#            ax=axd[plot_key])\n",
    "\n",
    "eval_result_df_mean_fixed=eval_result_df[\n",
    "    (eval_result_df[\"method\"]==\"fixed_answer_plus\")\n",
    "    &(eval_result_df[\"answer_length\"]!=0)\n",
    "    &(eval_result_df[\"rank_n\"]<=3)].groupby([\"rank_n\", \"model\"]).mean()\n",
    "print(eval_result_df_mean_fixed)\n",
    "sns.barplot(\n",
    "    x=\"rank_n\", y=\"metric\", hue=\"model\", hue_order=[\"MONET\", \"CLIP\"],\n",
    "    data=(eval_result_df_mean_fixed).reset_index(),\n",
    "    width=0.7,\n",
    "    ax=axd[plot_key]\n",
    ")\n",
    "\n",
    "\n",
    "# plt.title(\"\\\n",
    "# ground-truth is defined based on distribution of train/test set\\\n",
    "# \\n(i.e., similar to the `red` confounder in the ISIC)\")\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    axd[plot_key].spines[axis].set_linewidth(1.5)\n",
    "axd[plot_key].spines['right'].set_visible(False)\n",
    "axd[plot_key].spines['top'].set_visible(False)\n",
    "\n",
    "axd[plot_key].yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "# axd[plot_key].yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "axd[plot_key].yaxis.grid(True, which='major', linewidth=0.4, alpha=0.4)\n",
    "# axd[plot_key].yaxis.grid(True, which='minor', linewidth=0.2, alpha=0.4)\n",
    "\n",
    "axd[plot_key].set_ylim(0,1)\n",
    "\n",
    "axd[plot_key].tick_params(axis='both', which='major', labelsize=16)\n",
    "axd[plot_key].tick_params(axis='both', which='minor', labelsize=16)\n",
    "\n",
    "axd[plot_key].set_xlabel(\"Top-N\", fontsize=18)\n",
    "# axd[plot_key].set_ylabel(\"Freq. of recovering spurious corr.\\n(across all underperforming clusters)\", fontsize=16)\n",
    "axd[plot_key].set_ylabel(\"Freq. of recovering spurious corr.\", fontsize=18)\n",
    "\n",
    "for patch in axd[plot_key].patches :\n",
    "    patch.set_linewidth(1)\n",
    "    patch.set_edgecolor(\"black\")\n",
    "\n",
    "\n",
    "    \n",
    "# leg=axd[plot_key].legend(fontsize = 16, facecolor='white', framealpha=0.5)\n",
    "# axd[plot_key].get_legend().remove()\n",
    "leg=axd[plot_key].legend(fontsize = 16, facecolor='white', framealpha=0.5, ncols=2,\n",
    "                         loc='upper center', bbox_to_anchor=(0.487, -0.13, 0, 0), \n",
    "                        \n",
    "                        )\n",
    "leg.set_title(\"\", prop={\"size\":16})\n",
    "\n",
    "axd[plot_key].text(x=-0.2, \n",
    "                   y=1.09, \n",
    "#                    y=1.03, \n",
    "                   transform=axd[plot_key].transAxes,\n",
    "                     s=\" B.\", fontsize=23, weight='bold')\n",
    "# leg.set_title(\"Model\", prop={\"size\":16})\n",
    "\n",
    "\n",
    "# axd[plot_key].set_title(\"Do the top N rec spurious correlation\", fontsize=16)\n",
    "axd[plot_key].set_title(\"Do the top-N concept explanations recover spurious correlations?\\n(across all low-performing clusters)\", \n",
    "                        fontsize=18)\n",
    "\n",
    "plot_key=\"on_the_spot\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sns.barplot(x=\"rank_n\", y=\"metric_random_ratio\", hue=\"model\", \n",
    "# data=eval_result_df[\n",
    "#     (eval_result_df[\"method\"]==\"on_the_spot_plus\")\n",
    "#     &(eval_result_df[\"answer_length\"]!=0)\n",
    "#     &(eval_result_df[\"rank_n\"]<=3)], ax=axd[plot_key])\n",
    "\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    axd[plot_key].spines[axis].set_visible(False)\n",
    "    \n",
    "axd[plot_key].tick_params(left = False, right = False , labelleft = False ,\n",
    "            labelbottom = False, bottom = False)\n",
    "\n",
    "fig.savefig(log_dir/\"plots\"/\"model_audit_main_benchmark.png\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/\"model_audit_main_benchmark.jpg\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/\"model_audit_main_benchmark.svg\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/\"model_audit_main_benchmark.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4987426",
   "metadata": {},
   "source": [
    "# two ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9ab607",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"axes.prop_cycle\"]=cycler('color', [np.array(i)/256 for i in [Paired[12][1], \n",
    "                                                                                Paired[12][3],\n",
    "                                                                                Paired[12][5],\n",
    "                                                                                Paired[12][7],\n",
    "                                                                                Paired[12][9],\n",
    "                                                                                Paired[12][11]\n",
    "                                                                                ]])\n",
    "# fig = plt.figure(constrained_layout=True, figsize=(15, 6))\n",
    "fig = plt.figure(figsize=(18, 5))\n",
    "subfigs = fig.subfigures(1, 1)\n",
    "\n",
    "axes = subfigs.subplots(1,2, gridspec_kw={\"wspace\":0.3})\n",
    "\n",
    "axd={'fixed': axes[0], \"on_the_spot\": axes[1] }\n",
    "\n",
    "plot_key=\"fixed\"\n",
    "\n",
    "\n",
    "# sns.barplot(x=\"rank_n\", y=\"metric\", hue=\"model\", hue_order=[\"MONET\", \"CLIP\"], \n",
    "#             data=eval_result_df[(eval_result_df[\"method\"]==\"fixed_answer_plus\")&(eval_result_df[\"rank_n\"]<=3)],\n",
    "#            ax=axd[plot_key])\n",
    "\n",
    "eval_result_df_mean_fixed=eval_result_df[\n",
    "    (eval_result_df[\"method\"]==\"fixed_answer_plus\")\n",
    "    &(eval_result_df[\"answer_length\"]!=0)\n",
    "    &(eval_result_df[\"rank_n\"]<=3)].groupby([\"rank_n\", \"model\"]).mean()\n",
    "print(eval_result_df_mean_fixed)\n",
    "sns.barplot(\n",
    "    x=\"rank_n\", y=\"metric\", hue=\"model\", hue_order=[\"MONET\", \"CLIP\"],\n",
    "    data=(eval_result_df_mean_fixed).reset_index(),\n",
    "    width=0.7,\n",
    "    ax=axd[plot_key]\n",
    ")\n",
    "\n",
    "\n",
    "# plt.title(\"\\\n",
    "# ground-truth is defined based on distribution of train/test set\\\n",
    "# \\n(i.e., similar to the `red` confounder in the ISIC)\")\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    axd[plot_key].spines[axis].set_linewidth(1.5)\n",
    "axd[plot_key].spines['right'].set_visible(False)\n",
    "axd[plot_key].spines['top'].set_visible(False)\n",
    "\n",
    "axd[plot_key].yaxis.set_major_locator(MultipleLocator(0.1))\n",
    "# axd[plot_key].yaxis.set_minor_locator(MultipleLocator(0.1))\n",
    "axd[plot_key].yaxis.grid(True, which='major', linewidth=0.4, alpha=0.4)\n",
    "# axd[plot_key].yaxis.grid(True, which='minor', linewidth=0.2, alpha=0.4)\n",
    "\n",
    "axd[plot_key].set_ylim(0,1)\n",
    "\n",
    "axd[plot_key].tick_params(axis='both', which='major', labelsize=16)\n",
    "axd[plot_key].tick_params(axis='both', which='minor', labelsize=16)\n",
    "\n",
    "axd[plot_key].set_xlabel(\"Top-N\", fontsize=18)\n",
    "# axd[plot_key].set_ylabel(\"Freq. of recovering spurious corr.\\n(across all underperforming clusters)\", fontsize=16)\n",
    "axd[plot_key].set_ylabel(\"Freq. of recovering spurious corr.\", fontsize=18)\n",
    "\n",
    "for patch in axd[plot_key].patches :\n",
    "    patch.set_linewidth(1)\n",
    "    patch.set_edgecolor(\"black\")\n",
    "\n",
    "\n",
    "    \n",
    "# leg=axd[plot_key].legend(fontsize = 16, facecolor='white', framealpha=0.5)\n",
    "axd[plot_key].get_legend().remove()\n",
    "axd[plot_key].text(x=-0.2, \n",
    "                   y=1.09, \n",
    "#                    y=1.03, \n",
    "                   transform=axd[plot_key].transAxes,\n",
    "                     s=\" B.\", fontsize=23, weight='bold')\n",
    "# leg.set_title(\"Model\", prop={\"size\":16})\n",
    "\n",
    "\n",
    "# axd[plot_key].set_title(\"Do the top N rec spurious correlation\", fontsize=16)\n",
    "axd[plot_key].set_title(\"Do the top-N concept explanations recover spurious correlations?\\n(across all low-performing clusters)\", \n",
    "                        fontsize=18)\n",
    "\n",
    "plot_key=\"on_the_spot\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# sns.barplot(x=\"rank_n\", y=\"metric_random_ratio\", hue=\"model\", \n",
    "# data=eval_result_df[\n",
    "#     (eval_result_df[\"method\"]==\"on_the_spot_plus\")\n",
    "#     &(eval_result_df[\"answer_length\"]!=0)\n",
    "#     &(eval_result_df[\"rank_n\"]<=3)], ax=axd[plot_key])\n",
    "\n",
    "eval_result_df_mean_spot=eval_result_df[\n",
    "    (eval_result_df[\"method\"]==\"on_the_spot_plus\")\n",
    "    &(eval_result_df[\"answer_length\"]!=0)\n",
    "    &(eval_result_df[\"rank_n\"]<=3)].groupby([\"rank_n\", \"model\"]).mean()\n",
    "print(eval_result_df_mean_spot)\n",
    "sns.barplot(\n",
    "    x=\"rank_n\", y=0, hue=\"model\", hue_order=[\"MONET\", \"CLIP\"],\n",
    "    data=(eval_result_df_mean_spot[\"metric\"]/eval_result_df_mean_spot[\"random_performance\"]).reset_index(),\n",
    "    width=0.7,\n",
    "    ax=axd[plot_key]\n",
    ")\n",
    "\n",
    "for axis in ['top','bottom','left','right']:\n",
    "    axd[plot_key].spines[axis].set_linewidth(1.5)\n",
    "axd[plot_key].spines['right'].set_visible(False)\n",
    "axd[plot_key].spines['top'].set_visible(False)\n",
    "\n",
    "axd[plot_key].yaxis.set_major_locator(MultipleLocator(1))\n",
    "axd[plot_key].yaxis.set_minor_locator(MultipleLocator(0.5))\n",
    "axd[plot_key].yaxis.grid(True, which='major', linewidth=0.4, alpha=0.4)\n",
    "axd[plot_key].yaxis.grid(True, which='minor', linewidth=0.2, alpha=0.4)\n",
    "\n",
    "axd[plot_key].tick_params(axis='both', which='major', labelsize=16)\n",
    "axd[plot_key].tick_params(axis='both', which='minor', labelsize=16)\n",
    "\n",
    "axd[plot_key].set_xlabel(\"Top-N\", fontsize=18)\n",
    "# axd[plot_key].set_ylabel(\"Prob. of listing ground-truth concept\\ncompared to random (per cluster)\", fontsize=16)\n",
    "axd[plot_key].set_ylabel(\"Ratio of freq. of including ground truth\\nto that in random ordering\", fontsize=18)\n",
    "\n",
    "\n",
    "\n",
    "for patch in axd[plot_key].patches :\n",
    "    patch.set_linewidth(1)\n",
    "    patch.set_edgecolor(\"black\")\n",
    "\n",
    "leg=axd[plot_key].legend(fontsize = 16, facecolor='white', framealpha=0.5, ncols=2,\n",
    "                         loc='upper center', bbox_to_anchor=(-0.2, -0.1, 0, 0)\n",
    "                        \n",
    "                        )\n",
    "leg.set_title(\"\", prop={\"size\":16})\n",
    "axd[plot_key].text(x=-0.19, \n",
    "                   y=1.09, \n",
    "#                    y=1.03, \n",
    "                   transform=axd[plot_key].transAxes,\n",
    "                     s=\"C.\", fontsize=23, weight='bold')\n",
    "# plt.tight_figure()\n",
    "# axd[plot_key].set_title(\"Ground-truth\", fontsize=16)\n",
    "axd[plot_key].set_title(\"Do the top-N concept explanations include ground truth\\ndefined per low-performing cluster?\", \n",
    "                        fontsize=18)\n",
    "\n",
    "fig.savefig(log_dir/\"plots\"/\"model_audit_main_benchmark.png\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/\"model_audit_main_benchmark.jpg\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/\"model_audit_main_benchmark.svg\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/\"model_audit_main_benchmark.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1806cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_result_df_mean_spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562fbcdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n=3\n",
    "num_true=2\n",
    "1-(math.comb(48-num_true, top_n) / math.comb(48, top_n))**20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc30d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_dict_isic=torch.load(\"logs/experiment_results/data_audit_new_0429.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6a85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_dict_isic[\"isic\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e824b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_1=\"ViDIR Group, Department of Dermatology, Medical University of Vienna\"\n",
    "hospital_2=\"Hospital Clnic de Barcelona\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6f2e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_f1_thres_isic={}\n",
    "\n",
    "classifier_val_idx=variable_dict_isic[dataset_name][f\"classifier_dataloader_{hospital_1}\"][1].dataset.metadata_all.index\n",
    "y_test=variable_dict_isic[dataset_name][\"classifier_dataloader_all\"].dataset.metadata_all[\"label\"].loc[classifier_val_idx]\n",
    "y_test_predicted_probas=variable_dict_isic[dataset_name][f\"classifier_model_{hospital_1}_eval\"][\"logits\"].loc[classifier_val_idx]\n",
    "# y_test_predicted_probas=y_test_predicted_probas.map(lambda x: 1/(1 + np.exp(-x)))\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(y_test, y_test_predicted_probas)\n",
    "numerator = 2 * recall * precision\n",
    "denom = recall + precision\n",
    "f1_scores = np.divide(numerator, denom, out=np.zeros_like(denom), where=(denom!=0))\n",
    "max_f1 = np.max(f1_scores)\n",
    "max_f1_thresh = thresholds[np.argmax(f1_scores)]\n",
    "max_f1_thres_isic[hospital_1]=max_f1_thresh\n",
    "print(max_f1_thresh)\n",
    "\n",
    "classifier_val_idx=variable_dict_isic[dataset_name][f\"classifier_dataloader_{hospital_2}\"][1].dataset.metadata_all.index\n",
    "y_test=variable_dict_isic[dataset_name][\"classifier_dataloader_all\"].dataset.metadata_all[\"label\"].loc[classifier_val_idx]\n",
    "y_test_predicted_probas=variable_dict_isic[dataset_name][f\"classifier_model_{hospital_2}_eval\"][\"logits\"].loc[classifier_val_idx]\n",
    "# y_test_predicted_probas=y_test_predicted_probas.map(lambda x: 1/(1 + np.exp(-x)))\n",
    "\n",
    "precision, recall, thresholds = sklearn.metrics.precision_recall_curve(y_test, y_test_predicted_probas)\n",
    "numerator = 2 * recall * precision\n",
    "denom = recall + precision\n",
    "f1_scores = np.divide(numerator, denom, out=np.zeros_like(denom), where=(denom!=0))\n",
    "max_f1 = np.max(f1_scores)\n",
    "max_f1_thresh = thresholds[np.argmax(f1_scores)]\n",
    "max_f1_thres_isic[hospital_2]=max_f1_thresh\n",
    "\n",
    "print(max_f1_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e5f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subset_index(dataset_name, metadata_all, attribution):\n",
    "    if dataset_name==\"isic\":\n",
    "        if attribution==\"all\":\n",
    "            #pd.Series([True], index=metadata_all)\n",
    "            subset_idx=np.array([True]*len(metadata_all))\n",
    "        else:\n",
    "            collection_65=(metadata_all[\"collection_65\"]==1).values\n",
    "            \n",
    "            if attribution==\"barcelona_all\":\n",
    "                subset_idx=((metadata_all[\"attribution\"]==\"Department of Dermatology, Hospital Clnic de Barcelona\")|(metadata_all[\"attribution\"]==\"Hospital Clnic de Barcelona\")).values\n",
    "            elif attribution==\"mskcc_all\":\n",
    "                subset_idx=((metadata_all[\"attribution\"]==\"MSKCC\")|(metadata_all[\"attribution\"]==\"Memorial Sloan Kettering Cancer Center\")).values\n",
    "            else:\n",
    "                subset_idx=(metadata_all[\"attribution\"]==attribution).values          \n",
    "                \n",
    "            subset_idx=subset_idx&collection_65\n",
    "        #for attribution in [None]+[\"barcelona\", \"vienna\", \"barcelona_all\"]:\n",
    "        #for attribution in [\"all\"]+list(metadata_all[\"attribution\"].unique())+[\"barcelona_all\", \"mskcc_all\"]:\n",
    "  \n",
    "                    \n",
    "    return subset_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0303299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(dataset_name, idx):\n",
    "    if dataset_name==\"clinical_fd_clean\":\n",
    "        if idx in [1866, 3016, 2737, 2905, 982]:\n",
    "            return False\n",
    "        else:\n",
    "            return True    \n",
    "    elif dataset_name==\"fitzpatrick17k_clean_threelabel\":\n",
    "        if idx in [1866, 3016, 3227, 2949, 394, 2861, 1515, 2109, 1385]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e173093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_concept_name(dataset_name, concept_name):\n",
    "    if dataset_name==\"isic\":\n",
    "        if concept_name.startswith(\"disease\"):\n",
    "            return False\n",
    "        elif concept_name in [\"melanoma\", \"malignant\"]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "    else:\n",
    "        raise NotImplemented(dataset_name)\n",
    "\n",
    "def shorten_concept_name(concept_name, strict=True):\n",
    "    if concept_name.startswith(\"disease_\"):\n",
    "        short_name=concept_name.replace(\"disease_\", \"\")\n",
    "    elif concept_name==\"skincon_Erythema\":\n",
    "        short_name=\"Erythema\"\n",
    "    elif concept_name==\"skincon_Bulla\":\n",
    "        short_name=\"Bulla\"\n",
    "    elif concept_name==\"skincon_Lichenification\":\n",
    "        short_name=\"Lichenification\"\n",
    "    elif concept_name==\"skincon_Pustule\":\n",
    "        short_name=\"Pustule\"\n",
    "    elif concept_name==\"skincon_Ulcer\":\n",
    "        short_name=\"Ulcer\"\n",
    "    elif concept_name==\"skincon_Warty/Papillomatous\":\n",
    "        short_name=\"Warty\"\n",
    "    elif concept_name==\"skincon_White(Hypopigmentation)\":\n",
    "        short_name=\"Hypopigmentation\"\n",
    "    elif concept_name==\"skincon_Brown(Hyperpigmentation)\":\n",
    "        short_name=\"Hyperpigmentation\"\n",
    "    elif concept_name==\"skincon_Exophytic/Fungating\":\n",
    "        short_name=\"Fungating\"          \n",
    "    elif concept_name==\"purple pen\":\n",
    "        short_name=\"Purple pen\"\n",
    "    elif concept_name==\"nail\":\n",
    "        short_name=\"Nail\"  \n",
    "    elif concept_name==\"orange sticker\":\n",
    "        short_name=\"Orange sticker\"          \n",
    "    elif concept_name==\"hair\":\n",
    "        short_name=\"Hair\"          \n",
    "    elif concept_name==\"gel\":\n",
    "        short_name=\"Gel\"\n",
    "    elif concept_name==\"red\":\n",
    "        short_name=\"Red\"     \n",
    "    elif concept_name==\"dermoscope border\":\n",
    "        short_name=\"Dermoscopic border\"\n",
    "    elif concept_name==\"pinkish\":\n",
    "        short_name=\"Pinkish\"\n",
    "    else:\n",
    "        if concept_name.startswith(\"skincon_\"):\n",
    "            short_name=concept_name[8:]\n",
    "        else:\n",
    "            if strict:\n",
    "                raise NotImplementedError(concept_name)\n",
    "            else:\n",
    "                short_name=concept_name\n",
    "            \n",
    "    return short_name\n",
    "\n",
    "def shorten_hospital_name(hospital_name):\n",
    "    if hospital_name==\"Hospital Clnic de Barcelona\":\n",
    "        short_name=\"Hospital Clnic de Barcelona\"\n",
    "    elif hospital_name==\"ViDIR Group, Department of Dermatology, Medical University of Vienna\":\n",
    "        short_name=\"Medical University of Vienna\"    \n",
    "    return short_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829cf389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode as sci_mode\n",
    "def plot_slice_figure(data_dict,\n",
    "                      prompt_info, \n",
    "                      row_per_slice=30, \n",
    "                      example_per_row=30, \n",
    "                      normalize=True, \n",
    "                      show_small_box=True, \n",
    "                      print_alphabet=True,\n",
    "                      print_legend_color=True,\n",
    "                      print_legend_color_idx=2,\n",
    "                      print_legend_number=True,\n",
    "                      fontsize=32,\n",
    "                      slice_title_fontsize=32,\n",
    "                      skip_section=0,\n",
    "                      figure_title=None):\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    red_color=np.array([212,17,89]) #np.array((222,40,40))\n",
    "    green_color=np.array([26,133,255]) #np.array((40,200,40))\n",
    "    [31, 120, 180], [51, 160, 44]\n",
    "    # two_color=[np.array([90, 0, 220]), np.array([51, 160, 44])]\n",
    "    #two_color=[np.array([31, 120, 180]), np.array([51, 160, 44])]\n",
    "    two_color=[np.array([60, 50, 180]), np.array([51, 160, 44])]\n",
    "\n",
    "    \n",
    "    total_slices=(len([j for exp_name in data_dict.keys() for j in data_dict[exp_name][\"sample_list_list\"]]))\n",
    "    \n",
    "    fig = plt.figure(figsize=(3*(example_per_row), \n",
    "                              3*(row_per_slice)*total_slices+\\\n",
    "                              0.4*(len(data_dict)-1)+\\\n",
    "                              0.3*((total_slices-1)-(len(data_dict)-1))\n",
    "                             )\n",
    "                    )\n",
    "\n",
    "    box1 = gridspec.GridSpec(len(data_dict), 1,\n",
    "                             wspace=0.0,\n",
    "                             hspace=0.4)\n",
    "    \n",
    "    axd={}\n",
    "    for idx1, exp_name in enumerate(data_dict.keys()):\n",
    "        box2 = gridspec.GridSpecFromSubplotSpec(len(data_dict[exp_name][\"sample_list_list\"]), 1,\n",
    "                        subplot_spec=box1[idx1], wspace=0.0, hspace=0.3)\n",
    "\n",
    "        for idx2, (slice_assignment) in enumerate(data_dict[exp_name][\"sample_list_list\"]):\n",
    "            box3 = gridspec.GridSpecFromSubplotSpec(row_per_slice, example_per_row,\n",
    "                                                                    subplot_spec=box2[idx2], wspace=0, hspace=0.05)            \n",
    "#             if example_per_slice//10==1:\n",
    "#                 box3 = gridspec.GridSpecFromSubplotSpec(row_per_slice, example_per_row,\n",
    "#                                                         subplot_spec=box2[idx2], wspace=0, hspace=0.05)\n",
    "#             else:\n",
    "#                 box3 = gridspec.GridSpecFromSubplotSpec(row_per_slice, example_per_row,\n",
    "#                                                         subplot_spec=box2[idx2], wspace=0.05, hspace=0.15)\n",
    "            for rank_num in range(row_per_slice*example_per_row):\n",
    "                ax=plt.Subplot(fig, box3[rank_num])\n",
    "                fig.add_subplot(ax)\n",
    "\n",
    "                plot_key=(idx1, idx2, rank_num)\n",
    "                axd[plot_key]=ax   \n",
    "                \n",
    "    #dsdsd           \n",
    "    for idx1, exp_name in enumerate(data_dict.keys()):\n",
    "        \n",
    "        targets=data_dict[exp_name][\"targets\"]\n",
    "        preds=data_dict[exp_name][\"preds\"]\n",
    "        main_title=data_dict[exp_name][\"main_title\"]   \n",
    "                \n",
    "        \n",
    "        for idx2, (sample_list) in enumerate(data_dict[exp_name][\"sample_list_list\"]):\n",
    "            image_idx_list=variable_dict[dataset_name][\"metadata_all\"].index.get_indexer(sample_list)\n",
    "        \n",
    "            count=0\n",
    "            rank_num=0\n",
    "            while rank_num<min(row_per_slice*example_per_row, len(image_idx_list)):\n",
    "                if check_image(dataset_name, image_idx_list[count]):\n",
    "                    pass\n",
    "                else:\n",
    "                    count+=1\n",
    "                    continue\n",
    "                    \n",
    "                plot_key=(idx1, idx2, rank_num)\n",
    "                \n",
    "                item=variable_dict[dataset_name][\"dataloader\"].dataset.getitem(image_idx_list[count])\n",
    "                image=item[\"image\"]\n",
    "                axd[plot_key].imshow(image.resize((300, 300)))\n",
    "                \n",
    "                if show_small_box:\n",
    "\n",
    "                    \n",
    "                    if item[\"metadata\"][\"benign_malignant_bool\"]==True:\n",
    "                        axd[plot_key].scatter(x=[0.905], y=[0.905], s=650, \n",
    "                                       linewidths=1.5,\n",
    "                                       edgecolor=np.array((0,0,0, 120))/256,\n",
    "                                       #edgecolor=np.array((255,255,0, 120))/256,\n",
    "                                       color=red_color/256,\n",
    "                                       marker=\"s\",\n",
    "                                       transform=axd[plot_key].transAxes)     \n",
    "\n",
    "                    elif item[\"metadata\"][\"benign_malignant_bool\"]==False:\n",
    "                        axd[plot_key].scatter(x=[0.905], y=[0.905], s=650, \n",
    "                                   linewidths=1.5,\n",
    "                                   #edgecolor=np.array((0,0,0, 120))/256,\n",
    "                                   edgecolor=np.array((0,0,0, 120))/256,\n",
    "                                   color=green_color/256,\n",
    "                                   marker=\"s\",\n",
    "                                   transform=axd[plot_key].transAxes)                 \n",
    "\n",
    "                    x1=0.82\n",
    "                    x2=0.99\n",
    "                    if preds.loc[item[\"metadata\"].name]==1:\n",
    "                        axd[plot_key].fill([x1, x2, x2, x1], [x1, x2, x1, x1], \n",
    "                                           color=red_color/256,\n",
    "                                          transform=axd[plot_key].transAxes\n",
    "                                          )    \n",
    "                    else:\n",
    "                        axd[plot_key].fill([x1, x2, x2, x1], [x1, x2, x1, x1], \n",
    "                                           color=green_color/256,\n",
    "                                          transform=axd[plot_key].transAxes\n",
    "                                          )\n",
    "    #                     axd[plot_key].scatter(x=[0.99], y=[0.99], s=700, \n",
    "    #                                    linewidths=1.3,\n",
    "    # #                                    edgecolor=np.array((0,0,0, 120))/256,\n",
    "    #                                    color=np.array((40,200,40))/256,\n",
    "    #                                         #color=np.array((100,40,40))/256,\n",
    "    #                                    marker=6,\n",
    "    #                                    transform=axd[plot_key].transAxes)                         \n",
    "                else:\n",
    "                    axd[plot_key].set_title(item[\"metadata\"].name, fontsize=10)\n",
    "\n",
    "                axd[plot_key].set_xticks([])\n",
    "                axd[plot_key].set_yticks([])  \n",
    "                      \n",
    "\n",
    "                if rank_num==0:   \n",
    "                    #shorten_concept_name(concept_name)\n",
    "\n",
    "                    #axd[plot_key].set_ylabel(shorten_concept_name(concept_name), fontsize=30, zorder=-10)\n",
    "                    #axd[plot_key].set_ylabel(str(idx1), fontsize=30, zorder=-10)\n",
    "                    pass\n",
    "\n",
    "                if rank_num==99:\n",
    "                    diff_dict_df=pd.DataFrame(diff_dict)\n",
    "                    #print(diff_dict_df[\"concept_name\"])\n",
    "                    diff_dict_df=diff_dict_df[diff_dict_df[\"concept_name\"].map(lambda x: check_concept_name(\"isic\", x))]\n",
    "                    \n",
    "#                     print(diff_dict_df.sort_values(\"diff_score\", ascending=False).iloc[:][\"concept_name\"])\n",
    "                    #title=f\"{int(slice_mask.sum()):d} {(targets[slice_mask]==((prob[slice_mask]>0.5).astype(int))).mean():.2f} \"\n",
    "                    #title=', '.join(diff_dict_df.sort_values(\"diff_score\", ascending=False).iloc[:5][\"concept_name\"].map(shorten_concept_name).tolist())\n",
    "\n",
    "#                     concept_str=diff_dict_df.sort_values(\"diff_score\", ascending=False).iloc[:5][\"concept_name\"]\n",
    "                    concept_str=diff_dict_df.sort_values(\"concept_presence_score\", ascending=False).iloc[:5][\"concept_name\"]\n",
    "#                     concept_str=diff_dict_df.sort_values(\"slice_score\", ascending=False).iloc[:5][\"concept_name\"]\n",
    "                    concept_str=concept_str.map(shorten_concept_name)\n",
    "                    concept_str=\", \".join(concept_str.str.replace(\"skincon_\",\"\"))\n",
    "                    concept_str=concept_str\n",
    "                    \n",
    "                    print(concept_str)\n",
    "#                     print(diff_dict_df.sort_values(\"diff_score\", ascending=False).iloc[:20])\n",
    "                    print(diff_dict_df.sort_values(\"concept_presence_score\", ascending=False).iloc[:20])                    \n",
    "#                     print(diff_dict_df.sort_values(\"concept_presence_score\", ascending=False).iloc[:20])                    \n",
    "                    \n",
    "\n",
    "                    \n",
    "                    #title+= / Predicted Pos={(prob[slice_mask]>0.5).sum()} Neg={(prob[slice_mask]<0.5).sum()}\"\n",
    "                                      \n",
    "                    title= concept_str\n",
    "                        \n",
    "                    targets.loc[sample_list].sum()\n",
    "                    axd[plot_key].text(x=-0., y=1.1, transform=axd[plot_key].transAxes,\n",
    "                                         s=title, fontsize=25, color=\"black\",\n",
    "                                      \n",
    "#                                       bbox=dict(facecolor='white', edgecolor='red')\n",
    "                                      )   \n",
    "                if rank_num==9:\n",
    "                    \n",
    "                    label_str=f\"True Malignant: {targets.loc[sample_list].sum()} Neg={(1-targets.loc[sample_list]).sum()}\"\n",
    "                    predicted_str=f\" Pred +={(preds.loc[sample_list]==1).sum()} Neg={(preds.loc[sample_list]==0).sum()}\"                    \n",
    "                    \n",
    "#                     title= f\"Malignant: {targets[slice_mask].sum()}  {(preds[slice_mask]==1).sum()}   Benign: {(1-targets[slice_mask]).sum()}  {(prob[slice_mask]<0.5).sum()}\"\n",
    "                    title= f\"True: {targets.loc[sample_list].sum()} / {(1-targets.loc[sample_list]).sum()}  Pred: {(preds.loc[sample_list]==1).sum()} / {(preds.loc[sample_list]==0).sum()} \"\n",
    "                    targets.loc[sample_list].sum()\n",
    "                    axd[plot_key].text(x=1.0, y=1.1, transform=axd[plot_key].transAxes,\n",
    "                                         s=title, fontsize=25, color=\"black\",\n",
    "                                       horizontalalignment=\"right\",\n",
    "\n",
    "#                                       bbox=dict(facecolor='white', edgecolor='red')\n",
    "                                      )   \n",
    "                    \n",
    "                if print_legend_number and idx1==len(data_dict)-1 and idx2==len(data_dict[exp_name][\"sample_list_list\"])-1 and rank_num==example_per_row-1:\n",
    "                    \n",
    "                    title= f\"True: # Malignant / # Benign  Pred: # Malignant / # Benign\"\n",
    "                    targets.loc[sample_list].sum()\n",
    "                    axd[plot_key].text(x=1.0, y=-0.25, transform=axd[plot_key].transAxes,\n",
    "                                         s=title, fontsize=23, color=\"black\",\n",
    "                                       horizontalalignment=\"right\",\n",
    "\n",
    "#                                       bbox=dict(facecolor='white', edgecolor='red')\n",
    "                                      )                       \n",
    "                    \n",
    "                    pass\n",
    "                   \n",
    "                    \n",
    "#                       axd[plot_key].text(x=-0.3, y=1.05, transform=axd[plot_key].transAxes,\n",
    "#                                          s=[\"A\", \"B\", \"C\", \"D\", \"E\"][idx1], fontsize=35, weight='bold')\n",
    "\n",
    "                for axis in ['top','bottom','left','right']:\n",
    "                    axd[plot_key].spines[axis].set_linewidth(1)      \n",
    "#                 print('idx~~~~', idx1)\n",
    "                if rank_num==0 and idx1==0 and idx2==0 and figure_title is not None:\n",
    "                    axd[plot_key].text(x=-0.33, \n",
    "                                         #y=1.1, \n",
    "                                         y=1.4,\n",
    "                                         transform=axd[plot_key].transAxes,\n",
    "                                         s=figure_title[0], \n",
    "                                         fontsize=35, weight='bold')  \n",
    "                    axd[plot_key].text(x=0.01, \n",
    "                                         #y=1.1, \n",
    "                                         y=1.4, \n",
    "                                         transform=axd[plot_key].transAxes,\n",
    "                                         s=figure_title[1],\n",
    "                                         fontsize=fontsize)        \n",
    "                    \n",
    "                if rank_num==0 and idx2==0:\n",
    "                    if print_alphabet and skip_section+idx1<26:\n",
    "                        axd[plot_key].text(x=-0.3, \n",
    "                                             #y=1.1, \n",
    "                                             y=1.1, \n",
    "                                             transform=axd[plot_key].transAxes,\n",
    "                                             s=[\"A.\", \"B.\", \"C.\", \"D.\", \"E.\", \"F.\", \"G.\", \"H.\", \"I.\", \"J.\", \n",
    "                                                \"K.\", \"L.\", \"M.\", \"N.\", \"O.\", \"P.\", \"Q.\", \"R.\", \"S.\", \"T.\", \n",
    "                                                \"U.\", \"V.\", \"W.\", \"X.\", \"Y.\", \"Z.\"][skip_section+idx1], \n",
    "                                             fontsize=35, weight='bold')  \n",
    "                        axd[plot_key].text(x=0.05, \n",
    "                                             #y=1.1, \n",
    "                                             y=1.1, \n",
    "                                             transform=axd[plot_key].transAxes,\n",
    "                                             s=main_title[0], \n",
    "                                             fontsize=fontsize)\n",
    "                    else:\n",
    "#                         axd[plot_key].text(x=-0.3, \n",
    "#                                              #y=1.1, \n",
    "#                                              y=1.4, \n",
    "#                                              transform=axd[plot_key].transAxes,\n",
    "#                                              s=[\"A.\", \"B.\", \"C.\", \"D.\", \"E.\"][skip_section+idx1], \n",
    "#                                              fontsize=35, weight='bold')  \n",
    "                        axd[plot_key].text(x=0.0, \n",
    "                                             #y=1.1, \n",
    "                                             y=1.1, \n",
    "                                             transform=axd[plot_key].transAxes,\n",
    "                                             s=main_title[0], \n",
    "                                             fontsize=slice_title_fontsize)\n",
    "    \n",
    "    \n",
    "                if rank_num==0 and idx2==1:\n",
    "                    axd[plot_key].text(x=0.0, \n",
    "                                         #y=1.1, \n",
    "                                         y=1.1, \n",
    "                                         transform=axd[plot_key].transAxes,\n",
    "                                         s=main_title[1], \n",
    "                                         fontsize=slice_title_fontsize)    \n",
    "#                                            , weight='bold')                        \n",
    "                    \n",
    "  \n",
    "                \n",
    "                \n",
    "                \n",
    "                if print_legend_color and idx1==len(data_dict)-1 and idx2==len(data_dict[exp_name][\"sample_list_list\"])-1 and rank_num==print_legend_color_idx:\n",
    "\n",
    "                    legend_elements = [Line2D([0], [0], marker='o', color=(1,1,1,1), \n",
    "                                              markerfacecolor=np.array((200,40,40))/256, \n",
    "                                              markeredgecolor=np.array((0,0,0))/256, \n",
    "                                              markersize=30, \n",
    "                                              label=\"Maligant\"),\n",
    "                                       Line2D([0], [0], marker='X', color=(1,1,1,1), \n",
    "                                              markerfacecolor=np.array((40,200,40))/256, \n",
    "                                              markeredgecolor=np.array((0,0,0))/256, \n",
    "                                              markersize=30, label=\"Benign\"),]\n",
    "\n",
    "                    legend_elements = [Line2D([0], [0], marker='s', color=(1,1,1,1), \n",
    "                                              markerfacecolor=red_color/256, \n",
    "                                              markeredgecolor=np.array((0,0,0))/256, \n",
    "                                              markersize=30, \n",
    "                                              label=\"Maligant\"),\n",
    "                                       Line2D([0], [0], marker='s', color=(1,1,1,1), \n",
    "                                              markerfacecolor=green_color/256, \n",
    "                                              markeredgecolor=np.array((0,0,0))/256, \n",
    "                                              markersize=30, label=\"Benign   (Upper left: True, Lower right: Pred)\"),]        \n",
    "\n",
    "                    axd[plot_key].legend(handles=legend_elements, \n",
    "                                        ncol=2, \n",
    "                                        handlelength=3,\n",
    "                                        handletextpad=-0.1, \n",
    "                                        columnspacing=1.5,\n",
    "                                        fontsize=23,\n",
    "                                        loc='lower center', \n",
    "                                        bbox_to_anchor=(1, -0.45))  \n",
    "                    \n",
    "#                     axd[plot_key].legend(handles=legend_elements, \n",
    "#                                         ncol=2, \n",
    "#                                         handlelength=3,\n",
    "#                                         handletextpad=-0.1, \n",
    "#                                         columnspacing=1.5,\n",
    "#                                         fontsize=23,\n",
    "#                                         loc='lower center', \n",
    "#                                         bbox_to_anchor=(0, -0.45))                      \n",
    "                    \n",
    "                rank_num+=1\n",
    "                count+=1                      \n",
    "            \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535df12e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb48e26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0079991a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0a4671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78013f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921e0ee1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a8847c22",
   "metadata": {},
   "source": [
    "# run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3e3184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset(image_features_norm, metadata_all, \n",
    "                  logits, labels, subset_idx):\n",
    "    \n",
    "    image_features_norm_subset=image_features_norm[metadata_all.index.get_indexer(metadata_all[subset_idx].index)]    \n",
    "    \n",
    "    logits_subset=logits.iloc[logits.index.get_indexer(metadata_all[subset_idx].index)]\n",
    "    \n",
    "    label_subset=labels.iloc[labels.index.get_indexer(metadata_all[subset_idx].index)]\n",
    "    \n",
    "    return image_features_norm_subset, logits_subset, label_subset\n",
    "\n",
    "image_features_norm_subset_from1_to2, logits_subset_from1_to2, label_subset_from1_to2 = \\\n",
    "select_subset(image_features_norm=variable_dict_isic[\"isic\"][\"image_features_norm\"],\n",
    "             metadata_all=variable_dict_isic[\"isic\"][\"metadata_all\"],\n",
    "             logits=variable_dict_isic[\"isic\"][f\"classifier_model_{hospital_1}_eval\"][\"logits\"],\n",
    "             labels=variable_dict_isic[\"isic\"][\"classifier_dataloader_all\"].dataset.metadata_all[\"label\"],\n",
    "             subset_idx=get_subset_index(dataset_name=\"isic\", \n",
    "                                         metadata_all=variable_dict_isic[\"isic\"][\"metadata_all\"], \n",
    "                                         attribution=hospital_2)&(variable_dict_isic[\"isic\"][\"valid_idx\"])) \n",
    "\n",
    "# test_result_list_from1_to2_with_disease=cluster_concept_test_real(similarity_info=variable_dict[\"isic\"][\"similarity_matrix\"]\n",
    "#                                                      , \n",
    "#                           clustering_features=pd.DataFrame(variable_dict[\"isic\"][\"efficientnet_feature\"].numpy(),\n",
    "#                                                              index=variable_dict[\"isic\"][\"metadata_all\"].index,\n",
    "#                                                             ), \n",
    "#                           fixed_answer=[\"red\"],\n",
    "#                          labels=label_subset_from1_to2, \n",
    "#                           logits=logits_subset_from1_to2, \n",
    "#                           threshold=max_f1_thres_isic[hospital_1],\n",
    "#                          metric_diff=0.1,\n",
    "#                          n_clusters=80, random_state=42)\n",
    "\n",
    "test_result_list_from1_to2_concept_only=cluster_concept_test_real(similarity_info=variable_dict[\"isic\"][\"similarity_matrix\"]\n",
    "[variable_dict[\"isic\"][\"similarity_matrix\"].columns[variable_dict[\"isic\"][\"similarity_matrix\"].columns.map(lambda x: check_concept_name(\"isic\", x))]]\n",
    "                                                     , \n",
    "                          clustering_features=pd.DataFrame(variable_dict[\"isic\"][\"efficientnet_feature\"].numpy(),\n",
    "                                                             index=variable_dict[\"isic\"][\"metadata_all\"].index,\n",
    "                                                            ), \n",
    "                          fixed_answer=[\"red\"],\n",
    "                         labels=label_subset_from1_to2, \n",
    "                          logits=logits_subset_from1_to2, \n",
    "                          threshold=max_f1_thres_isic[hospital_1],\n",
    "                         metric_diff=0.1,\n",
    "                         n_clusters=80, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14599ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_subset(image_features_norm, metadata_all, \n",
    "                  logits, labels, subset_idx):\n",
    "    \n",
    "    image_features_norm_subset=image_features_norm[metadata_all.index.get_indexer(metadata_all[subset_idx].index)]    \n",
    "    \n",
    "    logits_subset=logits.iloc[logits.index.get_indexer(metadata_all[subset_idx].index)]\n",
    "    \n",
    "    label_subset=labels.iloc[labels.index.get_indexer(metadata_all[subset_idx].index)]\n",
    "    \n",
    "    return image_features_norm_subset, logits_subset, label_subset\n",
    "\n",
    "image_features_norm_subset_from2_to1, logits_subset_from2_to1, label_subset_from2_to1 = \\\n",
    "select_subset(image_features_norm=variable_dict_isic[\"isic\"][\"image_features_norm\"],\n",
    "             metadata_all=variable_dict_isic[\"isic\"][\"metadata_all\"],\n",
    "             logits=variable_dict_isic[\"isic\"][f\"classifier_model_{hospital_2}_eval\"][\"logits\"],\n",
    "             labels=variable_dict_isic[\"isic\"][\"classifier_dataloader_all\"].dataset.metadata_all[\"label\"],\n",
    "             subset_idx=get_subset_index(dataset_name=\"isic\", \n",
    "                                         metadata_all=variable_dict_isic[\"isic\"][\"metadata_all\"], \n",
    "                                         attribution=hospital_1)&(variable_dict_isic[\"isic\"][\"valid_idx\"])) \n",
    "\n",
    "test_result_list_from2_to1_concept_only=cluster_concept_test_real(similarity_info=variable_dict[\"isic\"][\"similarity_matrix\"]\n",
    "[variable_dict[\"isic\"][\"similarity_matrix\"].columns[variable_dict[\"isic\"][\"similarity_matrix\"].columns.map(lambda x: check_concept_name(\"isic\", x))]]\n",
    "                                                     , \n",
    "                          clustering_features=pd.DataFrame(variable_dict[\"isic\"][\"efficientnet_feature\"].numpy(),\n",
    "                                                             index=variable_dict[\"isic\"][\"metadata_all\"].index,\n",
    "                                                            ), \n",
    "                          fixed_answer=[\"red\"],\n",
    "                         labels=label_subset_from2_to1, \n",
    "                          logits=logits_subset_from2_to1, \n",
    "                          threshold=max_f1_thres_isic[hospital_2],\n",
    "                         metric_diff=0.1,\n",
    "                         n_clusters=40, random_state=42)\n",
    "\n",
    "test_result_list_from2_to1_with_disease=cluster_concept_test_real(similarity_info=variable_dict[\"isic\"][\"similarity_matrix\"]\n",
    "                                                     , \n",
    "                          clustering_features=pd.DataFrame(variable_dict[\"isic\"][\"efficientnet_feature\"].numpy(),\n",
    "                                                             index=variable_dict[\"isic\"][\"metadata_all\"].index,\n",
    "                                                            ), \n",
    "                          fixed_answer=[\"red\"],\n",
    "                         labels=label_subset_from2_to1, \n",
    "                          logits=logits_subset_from2_to1, \n",
    "                          threshold=max_f1_thres_isic[hospital_2],\n",
    "                         metric_diff=0.1,\n",
    "                         n_clusters=40, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a315cfab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0f07f55",
   "metadata": {},
   "source": [
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f9673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result[\"labels\"].sort_values(\"kmeans_dist\").index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04ceecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_thres=variable_dict[\"isic\"][\"similarity_matrix\"].quantile(0.5, axis=0)\n",
    "data_dict_main={}\n",
    "data_dict_supple={}\n",
    "count=0\n",
    "for test_result in test_result_list_from1_to2_concept_only:\n",
    "    print(test_result[\"statistics\"].sort_values(\"diff_magnitude\", ascending=False))\n",
    "    concept_name_list_plus=test_result[\"on_the_spot_plus_pred\"][:5]\n",
    "    sampe_list_plus=pd.concat([variable_dict[\"isic\"][\"similarity_matrix\"][concept_name_list_plus].sum(axis=1).loc[test_result[\"labels\"].index].rename(\"concept\"),\n",
    "test_result[\"labels\"]], axis=1).sort_values([\"accuracy\", \"concept\"], ascending=[True, False]).index\n",
    "#     sampe_list_plus=test_result[\"labels\"].sort_values(\"kmeans_dist\").index\n",
    "#     test_result[\"labels\"].sort_values(\"kmeans_dist\").index,\n",
    "#                                   test_result[\"labels_ref\"].sort_values(\"kmeans_dist\").index\n",
    "    sub_title_plus=\", \".join([shorten_concept_name(i, strict=False) for i in concept_name_list_plus])\n",
    "    \n",
    "    concept_name_list_minus=test_result[\"on_the_spot_minus_pred\"][:3]\n",
    "    concept_name_list_minus=test_result[\"statistics\"][(test_result[\"statistics\"][\"diff_magnitude\"]<0)&(test_result[\"statistics\"][\"mean_value\"]>0.3)].sort_values(\"diff_magnitude\", ascending=True).iloc[:5].index.tolist()\n",
    "    sub_title_minus=\", \".join([shorten_concept_name(i, strict=False) for i in concept_name_list_minus])  \n",
    "    \n",
    "\n",
    "    \n",
    "    if count<5:\n",
    "        data_dict_main[count]={\n",
    "            \"targets\": label_subset_from1_to2.astype(int),\n",
    "            \"preds\": (logits_subset_from1_to2>max_f1_thres_isic[hospital_1]).astype(int),\n",
    "    #         \"sample_list_list\": [test_result[\"labels\"].sort_values(\"kmeans_dist\").index,\n",
    "    # #                              test_result[\"labels_ref\"].sort_values(\"kmeans_dist\").index\n",
    "    #                             ],\n",
    "            \"sample_list_list\":[\n",
    "                sampe_list_plus\n",
    "            ],           \n",
    "            \"main_title\": [sub_title_plus, sub_title_minus],\n",
    "        #     \"slice_assignment_list\": [slice_assignment_from1_to2[:,slice_idx] for slice_idx in [3]],\n",
    "        }  \n",
    "    if count<15:\n",
    "        data_dict_supple[count]={\n",
    "            \"targets\": label_subset_from1_to2.astype(int),\n",
    "            \"preds\": (logits_subset_from1_to2>max_f1_thres_isic[hospital_1]).astype(int),\n",
    "    #         \"sample_list_list\": [test_result[\"labels\"].sort_values(\"kmeans_dist\").index,\n",
    "    # #                              test_result[\"labels_ref\"].sort_values(\"kmeans_dist\").index\n",
    "    #                             ],\n",
    "            \"sample_list_list\":[\n",
    "                sampe_list_plus\n",
    "            ],           \n",
    "            \"main_title\": [sub_title_plus, sub_title_minus],\n",
    "        #     \"slice_assignment_list\": [slice_assignment_from1_to2[:,slice_idx] for slice_idx in [3]],\n",
    "        }          \n",
    "        \n",
    "    count+=1\n",
    "    \n",
    "fig=plot_slice_figure(data_dict=data_dict_main,\n",
    "                  prompt_info=variable_dict[\"isic\"][\"prompt_info\"],\n",
    "                  example_per_row=5,\n",
    "                  row_per_slice=1,\n",
    "                  normalize=True, \n",
    "                  show_small_box=True,\n",
    "                  skip_section=0,\n",
    "                  print_alphabet=False,\n",
    "                  print_legend_number=False,\n",
    "                  print_legend_color=False,\n",
    "                      slice_title_fontsize=27,\n",
    "                  figure_title=(\"D. \", \"Trained at Med U. Vienna / Tested at Hosp. Barcelona \"))\n",
    "fig.savefig(log_dir/\"plots\"/f\"model_audit_from1_to2_main.png\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/f\"model_audit_from1_to2_main.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e0e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ed616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318ae314",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_dict_supple[5][\"preds\"].loc[data_dict_supple[5][\"sample_list_list\"][0]][\n",
    "data_dict_supple[5][\"targets\"].loc[data_dict_supple[5][\"sample_list_list\"][0]]==1      \n",
    "]==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb25797b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5572801f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8075ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(74-19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6a1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "(74-19)/74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809f3a07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260597a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6d857",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig=plot_slice_figure(data_dict=data_dict_supple,\n",
    "                  prompt_info=variable_dict[\"isic\"][\"prompt_info\"],\n",
    "                  example_per_row=10,\n",
    "                  row_per_slice=1,\n",
    "                  normalize=True, \n",
    "                  show_small_box=True,\n",
    "                  skip_section=0,\n",
    "                  print_alphabet=True,\n",
    "                  print_legend_number=True,\n",
    "                  print_legend_color=True,\n",
    "                  figure_title=None)\n",
    "fig.savefig(log_dir/\"plots\"/f\"model_audit_from1_to2_supple.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b5cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig=plot_slice_figure(data_dict={\"a\":data_dict_main[0]},\n",
    "                  prompt_info=variable_dict[\"isic\"][\"prompt_info\"],\n",
    "                  example_per_row=10,\n",
    "                  row_per_slice=1,\n",
    "                  normalize=True, \n",
    "                  show_small_box=True,\n",
    "                  skip_section=0,\n",
    "                  print_alphabet=False,\n",
    "                  print_legend_number=False,\n",
    "                  print_legend_color=True,\n",
    "                  print_legend_color_idx=4,\n",
    "                  figure_title=(\"E. \", \"Trained at Med U. Vienna / Tested at Hosp. Barcelona \"))\n",
    "fig.savefig(log_dir/\"plots\"/f\"model_audit_main_legend.png\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/f\"model_audit_main_legend.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc7d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af32d241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d9fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "similarity_thres=variable_dict[\"isic\"][\"similarity_matrix\"].quantile(0.5, axis=0)\n",
    "data_dict_main={}\n",
    "data_dict_supple={}\n",
    "count=0\n",
    "for test_result in test_result_list_from2_to1_concept_only:\n",
    "    print(test_result[\"statistics\"].sort_values(\"diff_magnitude\", ascending=False))\n",
    "    concept_name_list_plus=test_result[\"on_the_spot_plus_pred\"][:5]\n",
    "    sampe_list_plus=pd.concat([variable_dict[\"isic\"][\"similarity_matrix\"][concept_name_list_plus].sum(axis=1).loc[test_result[\"labels\"].index].rename(\"concept\"),\n",
    "test_result[\"labels\"]], axis=1).sort_values([\"accuracy\", \"concept\"], ascending=[True, False]).index\n",
    "#     sampe_list_plus=test_result[\"labels\"].sort_values(\"kmeans_dist\").index\n",
    "#     test_result[\"labels\"].sort_values(\"kmeans_dist\").index,\n",
    "#                                   test_result[\"labels_ref\"].sort_values(\"kmeans_dist\").index\n",
    "    sub_title_plus=\", \".join([shorten_concept_name(i, strict=False) for i in concept_name_list_plus])\n",
    "    \n",
    "    concept_name_list_minus=test_result[\"on_the_spot_minus_pred\"][:3]\n",
    "    concept_name_list_minus=test_result[\"statistics\"][(test_result[\"statistics\"][\"diff_magnitude\"]<0)&(test_result[\"statistics\"][\"mean_value\"]>0.3)].sort_values(\"diff_magnitude\", ascending=True).iloc[:5].index.tolist()\n",
    "    sub_title_minus=\", \".join([shorten_concept_name(i, strict=False) for i in concept_name_list_minus])  \n",
    "    \n",
    "\n",
    "    \n",
    "    if count<5:\n",
    "        data_dict_main[count]={\n",
    "            \"targets\": label_subset_from2_to1.astype(int),\n",
    "            \"preds\": (logits_subset_from2_to1>max_f1_thres_isic[hospital_2]).astype(int),\n",
    "    #         \"sample_list_list\": [test_result[\"labels\"].sort_values(\"kmeans_dist\").index,\n",
    "    # #                              test_result[\"labels_ref\"].sort_values(\"kmeans_dist\").index\n",
    "    #                             ],\n",
    "            \"sample_list_list\":[\n",
    "                sampe_list_plus\n",
    "            ],           \n",
    "            \"main_title\": [sub_title_plus, sub_title_minus],\n",
    "        #     \"slice_assignment_list\": [slice_assignment_from1_to2[:,slice_idx] for slice_idx in [3]],\n",
    "        }  \n",
    "    if count<15:\n",
    "        data_dict_supple[count]={\n",
    "            \"targets\": label_subset_from2_to1.astype(int),\n",
    "            \"preds\": (logits_subset_from2_to1>max_f1_thres_isic[hospital_2]).astype(int),\n",
    "    #         \"sample_list_list\": [test_result[\"labels\"].sort_values(\"kmeans_dist\").index,\n",
    "    # #                              test_result[\"labels_ref\"].sort_values(\"kmeans_dist\").index\n",
    "    #                             ],\n",
    "            \"sample_list_list\":[\n",
    "                sampe_list_plus\n",
    "            ],           \n",
    "            \"main_title\": [sub_title_plus, sub_title_minus],\n",
    "        #     \"slice_assignment_list\": [slice_assignment_from1_to2[:,slice_idx] for slice_idx in [3]],\n",
    "        }          \n",
    "        \n",
    "    count+=1\n",
    "    \n",
    "fig=plot_slice_figure(data_dict=data_dict_main,\n",
    "                  prompt_info=variable_dict[\"isic\"][\"prompt_info\"],\n",
    "                  example_per_row=5,\n",
    "                  row_per_slice=1,\n",
    "                  normalize=True, \n",
    "                  show_small_box=True,\n",
    "                  skip_section=0,\n",
    "                  print_alphabet=False,\n",
    "                  print_legend_number=False,\n",
    "                  print_legend_color=False,\n",
    "                  slice_title_fontsize=27,\n",
    "                  figure_title=(\"E. \", \"Trained at Hosp. Barcelona / Tested at Med U. Vienna\"))\n",
    "fig.savefig(log_dir/\"plots\"/f\"model_audit_from2_to1_main.png\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/f\"model_audit_from2_to1_main.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0979a123",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_dict_supple[0][\"preds\"].loc[data_dict_supple[0][\"sample_list_list\"][0]][   \n",
    "data_dict_supple[0][\"targets\"].loc[data_dict_supple[0][\"sample_list_list\"][0]]==0   \n",
    "]==1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e2204",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data_dict_supple[0][\"targets\"].loc[data_dict_supple[0][\"sample_list_list\"][0]]==0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3740abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0f1da5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df17713",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig=plot_slice_figure(data_dict=data_dict_supple,\n",
    "                  prompt_info=variable_dict[\"isic\"][\"prompt_info\"],\n",
    "                  example_per_row=10,\n",
    "                  row_per_slice=1,\n",
    "                  normalize=True, \n",
    "                  show_small_box=True,\n",
    "                  skip_section=0,\n",
    "                  print_alphabet=True,\n",
    "                  print_legend_number=True,\n",
    "                  print_legend_color=True,\n",
    "                  figure_title=None)\n",
    "fig.savefig(log_dir/\"plots\"/f\"model_audit_from2_to1_supple.png\", bbox_inches='tight')\n",
    "fig.savefig(log_dir/\"plots\"/f\"model_audit_from2_to1_supple.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa2edfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f31dbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d670c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33616f68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1367493",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0108cb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74532b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c950d801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c565d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677ad62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.concat([test_result[\"statistics\"][\"diff_magnitude\"] for test_result in test_result_list_from1_to2_concept_only[:10]],\n",
    "axis=1).sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566eb8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([test_result[\"statistics\"][\"diff_magnitude\"] for test_result in test_result_list_from2_to1_concept_only],\n",
    "axis=1).sum(axis=1).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"notebooks/untitled.txt\", \"r\") as f:\n",
    "    f_lines=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39193bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lines=[i for i in f_lines if not i.startswith(\"100%\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7fd8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lines_valid=[]\n",
    "for i in range(len(f_lines)):\n",
    "    \n",
    "    if i+1<=len(f_lines)-1 and f_lines[i+1].startswith(\"break\"):\n",
    "        f_lines_valid.append(f_lines[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309e1abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([float(i.split(\"AUROC:\")[-1].strip()) for i in f_lines_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda66f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_lines_test=[i for i in f_lines if i.startswith(\"Test loss\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8ad8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([float(i.split(\"AUROC:\")[-1].strip()) for i in f_lines_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d49347f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MONET",
   "language": "python",
   "name": "monet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
