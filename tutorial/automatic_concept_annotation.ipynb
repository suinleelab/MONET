{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "667ffaa2",
   "metadata": {},
   "source": [
    "# Automatic concept annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e4a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "import clip\n",
    "import pandas as pd\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotImageClassification\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from PIL import Image\n",
    "import scipy\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e314507c",
   "metadata": {},
   "source": [
    "# Utils funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28157286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    A custom dataset class for loading images and associated metadata.\n",
    "\n",
    "    Args:\n",
    "        image_path_list (list): A list of file paths to the images.\n",
    "        transform (callable): A function/transform to apply to the images.\n",
    "        metadata_df (pandas.DataFrame, optional): A pandas DataFrame containing metadata for the images.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If the length of `image_path_list` is not equal to the length of `metadata_df`.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the image and metadata (if available) for a given index.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, image_path_list, transform, metadata_df=None):\n",
    "        self.image_path_list = image_path_list\n",
    "        self.transform = transform\n",
    "        self.metadata_df = metadata_df\n",
    "\n",
    "        if self.metadata_df is None:\n",
    "            self.metadata_df = pd.Series(index=self.image_path_list)\n",
    "        else:\n",
    "            assert len(self.image_path_list) == len(\n",
    "                self.metadata_df\n",
    "            ), \"image_path_list and metadata_df must have the same length\"\n",
    "            self.metadata_df.index = self.image_path_list\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_path_list[idx])\n",
    "\n",
    "        ret = {\"image\": self.transform(image)}\n",
    "\n",
    "        if self.metadata_df is not None:\n",
    "            ret.update({\"metadata\": self.metadata_df.iloc[idx]})\n",
    "\n",
    "        return ret\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6634103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    \"\"\"Custom collate function for the dataloader.\n",
    "\n",
    "    Args:\n",
    "        batch (list): list of dictionaries, each dictionary is a batch of data\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of collated data\n",
    "    \"\"\"\n",
    "\n",
    "    ret = {}\n",
    "    for key in batch[0]:\n",
    "        if isinstance(batch[0][key], pd.Series):\n",
    "            try:\n",
    "                ret[key] = pd.concat([d[key] for d in batch], axis=1).T\n",
    "            except RuntimeError:\n",
    "                raise RuntimeError(f\"Error while concatenating {key}\")\n",
    "        else:\n",
    "            try:\n",
    "                ret[key] = torch.utils.data.dataloader.default_collate(\n",
    "                    [d[key] for d in batch]\n",
    "                )\n",
    "            except RuntimeError:\n",
    "                raise RuntimeError(f\"Error while concatenating {key}\")\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def custom_collate_per_key(batch_all):\n",
    "    \"\"\"Custom collate function batched outputs.\n",
    "\n",
    "    Args:\n",
    "        batch_all (dict): dictionary of lists of objects, each dictionary is a batch of data\n",
    "    Returns:\n",
    "        dict: dictionary of collated data\n",
    "    \"\"\"\n",
    "\n",
    "    ret = {}\n",
    "    for key in batch_all:\n",
    "        if isinstance(batch_all[key][0], pd.DataFrame):\n",
    "            ret[key] = pd.concat(batch_all[key], axis=0)\n",
    "        elif isinstance(batch_all[key][0], torch.Tensor):\n",
    "            ret[key] = torch.concat(batch_all[key], axis=0)\n",
    "        else:\n",
    "            print(f\"Collating {key}...\")\n",
    "            ret[key] = torch.utils.data.dataloader.default_collate(\n",
    "                [elem for batch in tqdm.tqdm(batch_all[key]) for elem in batch]\n",
    "            )\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def dataloader_apply_func(\n",
    "    dataloader, func, collate_fn=custom_collate_per_key, verbose=True\n",
    "):\n",
    "    \"\"\"Apply a function to a dataloader.\n",
    "\n",
    "    Args:\n",
    "        dataloader (torch.utils.data.DataLoader): torch dataloader\n",
    "        func (function): function to apply to each batch\n",
    "        collate_fn (function, optional): collate function. Defaults to custom_collate_batch.\n",
    "\n",
    "    Returns:\n",
    "        dict: dictionary of outputs\n",
    "    \"\"\"\n",
    "    func_out_dict = {}\n",
    "\n",
    "    for batch in tqdm.tqdm(dataloader):\n",
    "        for key, func_out in func(batch).items():\n",
    "            func_out_dict.setdefault(key, []).append(func_out)\n",
    "\n",
    "    return collate_fn(func_out_dict)\n",
    "\n",
    "def get_transform(n_px):\n",
    "    def convert_image_to_rgb(image):\n",
    "        return image.convert(\"RGB\")\n",
    "    return T.Compose(\n",
    "        [\n",
    "            T.Resize(n_px, interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.CenterCrop(n_px),\n",
    "            convert_image_to_rgb,\n",
    "            T.ToTensor(),\n",
    "            T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),        \n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd1fe1a",
   "metadata": {},
   "source": [
    "# Model initilaize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d26858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the GPU device to use\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model_api = \"clip\"\n",
    "\n",
    "if model_api==\"clip\":\n",
    "    # Load model using original clip implementation\n",
    "    model, preprocess = clip.load(\"ViT-L/14\", device=device, jit=False)[0], get_transform(n_px=224)\n",
    "    model.load_state_dict(torch.hub.load_state_dict_from_url(\"https://aimslab.cs.washington.edu/MONET/weight_clip.pt\"))\n",
    "    model.eval()\n",
    "    print(\"model was loaded using original clip implementation\")\n",
    "else:\n",
    "    # Load model using huggingface clip implementation\n",
    "    processor_hf = AutoProcessor.from_pretrained(\"chanwkim/monet\")\n",
    "    model_hf = AutoModelForZeroShotImageClassification.from_pretrained(\"chanwkim/monet\")\n",
    "    model_hf.to(device)\n",
    "    model_hf.eval()\n",
    "    print(\"model was loaded using huggingface clip implementation\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa79667",
   "metadata": {},
   "source": [
    "## Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the directory containing the images\n",
    "image_dir=\"data/fitzpatrick17k/images\"\n",
    "\n",
    "image_path_list = [Path(path) for path in glob.glob(str(Path(image_dir) / \"*\"))]\n",
    "image_path_list = [\n",
    "    image_path\n",
    "    for image_path in image_path_list\n",
    "    if image_path.suffix in [\".png\", \".jpg\", \".jpeg\"]\n",
    "]\n",
    "\n",
    "image_dataset = ImageDataset(\n",
    "    image_path_list, preprocess, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee2e25b",
   "metadata": {},
   "source": [
    "## Get image embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380147b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    image_dataset,\n",
    "    batch_size=32,\n",
    "    num_workers=4,\n",
    "    collate_fn=custom_collate,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "\n",
    "def batch_func(batch):\n",
    "    with torch.no_grad():\n",
    "        if model_api==\"clip\":\n",
    "            image_features = model.encode_image(batch[\"image\"].to(device))\n",
    "        else:\n",
    "            image_features = model_hf.get_image_features(batch[\"image\"].to(device))\n",
    "\n",
    "    return {\n",
    "        \"image_features\": image_features.detach().cpu(),\n",
    "        \"metadata\": batch[\"metadata\"],\n",
    "    }\n",
    "\n",
    "\n",
    "image_embedding = dataloader_apply_func(\n",
    "    dataloader=dataloader,\n",
    "    func=batch_func,\n",
    "    collate_fn=custom_collate_per_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e491fd18",
   "metadata": {},
   "source": [
    "## Get concept embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2969f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt_embedding(\n",
    "    concept_term_list=[],\n",
    "    prompt_template_list=[\n",
    "        \"This is skin image of {}\",\n",
    "        \"This is dermatology image of {}\",\n",
    "        \"This is image of {}\",\n",
    "    ],\n",
    "    prompt_ref_list=[\n",
    "        [\"This is skin image\"],\n",
    "        [\"This is dermatology image\"],\n",
    "        [\"This is image\"],\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate prompt embeddings for a concept\n",
    "\n",
    "    Args:\n",
    "        concept_term_list (list): List of concept terms that will be used to generate prompt target embeddings.\n",
    "        prompt_template_list (list): List of prompt templates.\n",
    "        prompt_ref_list (list): List of reference phrases.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the normalized prompt target embeddings and prompt reference embeddings.\n",
    "    \"\"\"\n",
    "    # target embedding\n",
    "    prompt_target = [\n",
    "        [prompt_template.format(term) for term in concept_term_list]\n",
    "        for prompt_template in prompt_template_list\n",
    "    ]\n",
    "    prompt_target_tokenized = [\n",
    "        clip.tokenize(prompt_list, truncate=True) for prompt_list in prompt_target\n",
    "    ]\n",
    "    with torch.no_grad():\n",
    "        prompt_target_embedding = torch.stack(\n",
    "            [\n",
    "                model.encode_text(prompt_tokenized.to(next(model.parameters()).device)).detach().cpu()\n",
    "                #model_hf.get_text_features(prompt_tokenized.to(next(model.parameters()).device)).detach().cpu()\n",
    "                for prompt_tokenized in prompt_target_tokenized\n",
    "            ]\n",
    "        )\n",
    "    prompt_target_embedding_norm = (\n",
    "        prompt_target_embedding / prompt_target_embedding.norm(dim=2, keepdim=True)\n",
    "    )\n",
    "\n",
    "    # reference embedding\n",
    "    prompt_ref_tokenized = [\n",
    "        clip.tokenize(prompt_list, truncate=True) for prompt_list in prompt_ref_list\n",
    "    ]\n",
    "    with torch.no_grad():\n",
    "        prompt_ref_embedding = torch.stack(\n",
    "            [\n",
    "                model.encode_text(prompt_tokenized.to(next(model.parameters()).device)).detach().cpu()\n",
    "                #model_hf.get_text_features(prompt_tokenized.to(next(model.parameters()).device)).detach().cpu()\n",
    "                for prompt_tokenized in prompt_ref_tokenized\n",
    "            ]\n",
    "        )\n",
    "    prompt_ref_embedding_norm = prompt_ref_embedding / prompt_ref_embedding.norm(\n",
    "        dim=2, keepdim=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"prompt_target_embedding_norm\": prompt_target_embedding_norm,\n",
    "        \"prompt_ref_embedding_norm\": prompt_ref_embedding_norm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd768b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the concept \"bullae\", we here use the terms \"bullae\" and \"blister\" to generate the prompt embedding.\n",
    "\n",
    "concept_embedding = get_prompt_embedding(concept_term_list=[\"bullae\", \"blister\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cc6ad7",
   "metadata": {},
   "source": [
    "## Calculate concept presence score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae99182",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_concept_presence_score(\n",
    "    image_features_norm,\n",
    "    prompt_target_embedding_norm,\n",
    "    prompt_ref_embedding_norm,\n",
    "    temp=1 / np.exp(4.5944),\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates the concept presence score based on the given image features and concept embeddings.\n",
    "\n",
    "    Args:\n",
    "        image_features_norm (numpy.Tensor): Normalized image features.\n",
    "        prompt_target_embedding_norm (torch.Tensor): Normalized concept target embedding.\n",
    "        prompt_ref_embedding_norm (torch.Tensor): Normalized concept reference embedding.\n",
    "        temp (float, optional): Temperature parameter for softmax. Defaults to 1 / np.exp(4.5944).\n",
    "\n",
    "    Returns:\n",
    "        np.array: Concept presence score.\n",
    "    \"\"\"\n",
    "\n",
    "    target_similarity = (\n",
    "        prompt_target_embedding_norm.float() @ image_features_norm.T.float()\n",
    "    )\n",
    "    ref_similarity = prompt_ref_embedding_norm.float() @ image_features_norm.T.float()\n",
    "\n",
    "    target_similarity_mean = target_similarity.mean(dim=[1])\n",
    "    ref_similarity_mean = ref_similarity.mean(axis=1)\n",
    "\n",
    "    concept_presence_score = scipy.special.softmax(\n",
    "        [target_similarity_mean.numpy() / temp, ref_similarity_mean.numpy() / temp],\n",
    "        axis=0,\n",
    "    )[0, :].mean(axis=0)\n",
    "\n",
    "    return concept_presence_score\n",
    "\n",
    "image_features_norm = image_embedding[\"image_features\"] / image_embedding[\n",
    "    \"image_features\"\n",
    "].norm(dim=1, keepdim=True)\n",
    "\n",
    "concept_presence_score = calculate_concept_presence_score(\n",
    "    image_features_norm=image_features_norm,\n",
    "    prompt_target_embedding_norm=concept_embedding[\"prompt_target_embedding_norm\"],\n",
    "    prompt_ref_embedding_norm=concept_embedding[\"prompt_ref_embedding_norm\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fb25c3",
   "metadata": {},
   "source": [
    "## Plot top 10 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b8c82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_per_concept = 10\n",
    "\n",
    "# Create a figure\n",
    "fig = plt.figure(figsize=(10 * 2, (example_per_concept // 10 + 1) * 2))\n",
    "\n",
    "# Main GridSpec (1 row, 1 column)\n",
    "main_gs = gridspec.GridSpec(1, 1, figure=fig)\n",
    "\n",
    "# Nested GridSpec within the first subplot of main GridSpec\n",
    "# Adjust rows based on examples per concept\n",
    "nested_gs = gridspec.GridSpecFromSubplotSpec(\n",
    "    example_per_concept // 10 + (1 if example_per_concept % 10 > 0 else 0),\n",
    "    10,\n",
    "    subplot_spec=main_gs[0],\n",
    "    wspace=0,\n",
    "    hspace=0.1,\n",
    ")\n",
    "\n",
    "# Dictionary to store axes for later use\n",
    "axd = {}\n",
    "\n",
    "for rank_num in range(example_per_concept):\n",
    "    ax = plt.Subplot(fig, nested_gs[rank_num])\n",
    "    fig.add_subplot(ax)\n",
    "\n",
    "    # Generate a simple pattern for demonstration\n",
    "    image = Image.open(\n",
    "        image_path_list[np.argsort(concept_presence_score)[::-1][rank_num]]\n",
    "    )\n",
    "    # Display the image\n",
    "    ax.imshow(preprocess.transforms[1](preprocess.transforms[0](image)))\n",
    "    ax.axis(\"off\")  # Remove axes for a cleaner look\n",
    "\n",
    "    # Example key, replace with your actual key\n",
    "    plot_key = rank_num\n",
    "    axd[plot_key] = ax\n",
    "\n",
    "    axd[plot_key].set_title(\n",
    "        f\"Rank: {rank_num + 1}\\nScore: {concept_presence_score[np.argsort(concept_presence_score)[::-1][rank_num]]:.2f}\",\n",
    "        fontsize=8,\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f643d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
